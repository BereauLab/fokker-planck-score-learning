{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#fokker-planck-score-learning-efficient-free-energy-estimation-under-periodic-boundary-conditions","title":"Fokker-Planck Score Learning: Efficient Free-Energy Estimation Under Periodic Boundary Conditions","text":"<p>This package contains a proof-of-concept implementation of the Fokker-Planck score learning approach.</p> <p>This package is published in:</p> <p>Fokker-Planck Score Learning: Efficient Free-Energy Estimation Under Periodic Boundary Conditions, D. Nagel, and T. Bereau, arXiv 2025, doi: 10.48550/arXiv.2506.15653</p> <p>We kindly ask you to cite this article in case you use this software package for published works.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>TBA</li> <li>Documentation including tutorials</li> <li>Supports Python 3.10-3.13</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#installation","title":"Installation","text":"<p>The package is called <code>fpsl</code> and will be soon available via PyPI. To install it, simply call: <pre><code>python3 -m pip install fpsl\n</code></pre> For now, you can install it from github. Download the repo and setup an env with with <code>fpsl</code> installed with <code>uv</code>. If you do not have <code>uv</code> you can get it here. <pre><code>uv sync --extra cuda  # if you have an Nvidia GPU\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":"<p>Add here a short example.</p> <pre><code>import fpsl\n\nddm = fps.DrivenDDM(\n    sigma_min=1e-3,\n    symmetric=True,\n    fourier_features=4,\n    ...,\n)\n# load x position of MD trajectory and forces f\nddm.train(\n    ...\n)\n...\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is inspired by Keep a Changelog, and Element and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#020-2025-08-06","title":"0.2.0 - 2025-08-06","text":""},{"location":"changelog/#added-features-and-improvements","title":"Added Features and Improvements \ud83d\ude4c:","text":"<ul> <li>Added CI for automated publishing to PyPi</li> <li>Added a tutorial with a brief theoretical recap, and an example application</li> <li>Improved documentation, with submodule docstrings</li> </ul>"},{"location":"changelog/#other-changes","title":"Other changes:","text":"<ul> <li>Cleaned up some code</li> <li>Add dependabot</li> </ul>"},{"location":"changelog/#bugfix","title":"Bugfix \ud83d\udc1b:","text":"<ul> <li>Fix using inner product when estimating work</li> </ul>"},{"location":"changelog/#010-2025-06-28","title":"0.1.0 - 2025-06-28","text":"<ul> <li>Initial release \ud83c\udf89</li> </ul>"},{"location":"contributing/","title":"Welcome to the <code>fpsl</code> Contributing Guide","text":"<p>This guide will give you an overview of the contribution workflow from opening an issue and creating a PR. To get an overview of the project, read the module overview.</p>"},{"location":"contributing/#issues","title":"Issues","text":""},{"location":"contributing/#create-a-new-issue","title":"Create a new issue","text":"<p>If you spot a bug, want to request a new functionality, or have a question on how to use the module, please search if an issue already exists. If a related issue does not exist, feel free to open a new issue.</p>"},{"location":"contributing/#solve-an-issue","title":"Solve an issue","text":"<p>If you want to contribute and do not how, feel free to scan through the existing issues.</p>"},{"location":"contributing/#create-a-new-pull-request","title":"Create a new pull request","text":""},{"location":"contributing/#create-a-fork","title":"Create a fork","text":"<p>If you want to request a change, you first have to fork the repository.</p>"},{"location":"contributing/#setup-a-development-environment","title":"Setup a development environment","text":"<p>It is recommended to use <code>uv</code> to set up the development environment. Run inside your forked repository <pre><code># for cpu\nuv sync --group docs\n\n# for cuda12\nuv sync --extra cuda --group docs\n\n# install pre-commit\nuv run pre-commit install\n</code></pre></p>"},{"location":"contributing/#make-changes-and-run-tests","title":"Make changes and run tests","text":"<p>Apply your changes and check if you followed the coding style (PEP8) by running <pre><code>uv run ruff check\n</code></pre> All errors pointing to <code>./build/</code> can be neglected.</p> <p>If you add a new function/method/class please ensure that you add a test function, as well. Running the test simply by <pre><code>uv run tox\n</code></pre> Ensure that the coverage does not decrease.</p>"},{"location":"contributing/#open-a-pull-request","title":"Open a pull request","text":"<p>Now you are ready to open a pull request and please do not forget to add a description.</p>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2025 Bereau Lab</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"maintenance/","title":"Welcome to the <code>fpsl</code> Maintenance Guideline","text":"<p>This guide will give you an overview of how to publish a new version of fpsl. In the following we will refer to the new version as <code>v1.*.*</code>. This needs to be substituted to the current version, e.g. <code>v1.1.3</code>.</p>"},{"location":"maintenance/#prepare-new-release","title":"Prepare New Release","text":"<ol> <li>the version number in <code>pyproject.toml</code> are bumped,</li> <li>a new tag is created via <code>git tag v1.*.*</code> and pushed <code>git push --tags</code>, and</li> <li>the changelog includes the new tag and all changes of the release.</li> </ol>"},{"location":"maintenance/#build-and-upload-to-pypi","title":"Build and Upload to PyPI","text":"<p>Todo: Describe how to build and upload to PyPI with using <code>uv</code> and Github action.</p>"},{"location":"reference/","title":"fpsl","text":"<p>Fokker-Planck Score Learning (FPSL): Score-based diffusion models for periodic data.</p> <p>FPSL is a Python package for training and sampling from score-based denoising diffusion models, with specialized support for periodic boundary conditions and force-conditioned generation. The package implements the Fokker-Planck Score Learning approach for learning score functions on toroidal (circular) domains.</p> <p>This package contains the following main components:</p> <ul> <li>ddm:     This module implements the core FPSL class for learning the     equilibrium free energy (PMF) from biased samples. It includes     neural network architectures, noise scheduling, prior distributions, and     force conditioning schedules for diffusion processes.</li> <li>datasets:     Collection of one-dimensional potential energy landscapes and biased-force     variants for testing and benchmarking diffusion models.</li> <li>utils:     Utility classes and functions including Gaussian mixture models, numerical     integrators for stochastic differential equations, and base classes.</li> </ul> <p>To get started, please have a look at the tutorials.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fpsl<ul> <li>datasets<ul> <li>datasets</li> <li>potentials</li> </ul> </li> <li>ddm<ul> <li>forceschedule</li> <li>models</li> <li>network</li> <li>noiseschedule</li> <li>prior</li> <li>priorschedule</li> </ul> </li> <li>utils<ul> <li>baseclass</li> <li>gmm</li> <li>integrators</li> <li>typing</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/datasets/","title":"datasets","text":"<p>This submodule provides a suite of one-dimensional potential energy landscapes and their corresponding biased-force variants for use in score-based and diffusion-based modeling experiments. Each dataset class encapsulates a specific analytic potential function and supporting machinery to generate samples, compute energies, and (where applicable) apply an external biasing force.</p> <p>The following [DataSet][fpsl.datasets.DataSet] classes are included:</p> <ul> <li>WPotential1D     A symmetric double-well (W-shaped) potential in one dimension.</li> <li>BiasedForceWPotential1D     The WPotential1D with an added constant biasing force term.</li> <li>ToyMembranePotential1D     A simple membrane-like potential featuring a central barrier and flanking wells.</li> <li>BiasedForceToyMembranePotential1D     The ToyMembranePotential1D augmented with an external biasing force.</li> <li>ToyMembrane2Potential1D     An extended membrane potential with two barriers and three wells.</li> <li>BiasedForceToyMembrane2Potential1D     The ToyMembrane2Potential1D with an additional biasing force.</li> <li>ToyMembrane3Potential1D     A higher-order membrane potential featuring three barriers and four wells.</li> <li>BiasedForceToyMembrane3Potential1D     The ToyMembrane3Potential1D augmented with an external biasing force.</li> </ul> <p>All classes expose a consistent interface for:     \u2022 Sampling data points from the potential's Boltzmann distribution.     \u2022 Computing potential energies and (optional) biasing forces.     \u2022 Integrating seamlessly with score-based learning workflows.</p> <p>Usage example:</p> <pre><code>dataset = WPotential1D(num_samples=10000, temperature=1.0)\nx, energy = dataset.sample()\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.BiasedForceWPotential1D","title":"<code>BiasedForceWPotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Biased-force dataset for the 1D W-potential.</p> <p>Extends WPotential1D by adding a bias force \\(b(x,t)\\).</p> <p>Parameters:</p> <ul> <li> <code>bias</code>               (<code>callable</code>)           \u2013            <p>Bias force function \\(b(x,t)\\).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>sample</code>             \u2013              <p>Simulate biased dynamics and return samples.</p> </li> </ul>"},{"location":"reference/datasets/#fpsl.datasets.BiasedForceWPotential1D.sample","title":"<code>sample(key, dt=0.0001, n_steps=int(100000.0), n_samples=2048, beta=1.0)</code>","text":"<p>Sample positions with bias force via Euler-Maruyama.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    dt: Float[ArrayLike, ''] = 1e-4,\n    n_steps: Int[ArrayLike, ''] = int(1e5),\n    n_samples: Int[ArrayLike, ''] = 2048,\n    beta: Float[ArrayLike, ''] = 1.0,\n) -&gt; Float[ArrayLike, 'n_samples 1']:\n    \"\"\"Sample positions with bias force via Euler-Maruyama.\"\"\"\n    integrator = BiasedForceEulerMaruyamaIntegrator(\n        bias_force=self.bias,\n        potential=self.potential,\n        n_dims=1,\n        dt=dt,\n        beta=beta,\n        n_heatup=n_steps,\n        gamma=self.gamma,\n    )\n    key1, key2 = jax.random.split(key)\n    trajs, _, _ = integrator.integrate(\n        key=key1,\n        X=jax.random.uniform(key2, (n_samples, 1)),\n        n_steps=0,\n    )\n    return trajs[-1] % 1\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.BiasedForceToyMembranePotential1D","title":"<code>BiasedForceToyMembranePotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembranePotential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the toy membrane potential.</p>"},{"location":"reference/datasets/#fpsl.datasets.BiasedForceToyMembrane2Potential1D","title":"<code>BiasedForceToyMembrane2Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembrane2Potential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the second toy membrane potential.</p>"},{"location":"reference/datasets/#fpsl.datasets.BiasedForceToyMembrane3Potential1D","title":"<code>BiasedForceToyMembrane3Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembrane3Potential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the third toy membrane potential.</p>"},{"location":"reference/datasets/#fpsl.datasets.WPotential1D","title":"<code>WPotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataSet</code></p> <p>Dataset for the 1D W-potential.</p> <p>This class integrates the 1D W-potential \\(U(x)\\) using the overdamped Langevin dynamics (Euler-Maruyama).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array_like, shape(dim1))</code>, default:                   <code>linspace(0,1,100)</code> )           \u2013            <p>Grid points for plotting the potential.</p> </li> <li> <code>gamma</code>               (<code>callable</code>, default:                   <code>lambda x: 1.0</code> )           \u2013            <p>Friction function \\(\\gamma(x)\\), defaults to constant 1.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>potential</code>             \u2013              <p>Returns \\(U(x)\\) from w_potential_1d.</p> </li> <li> <code>plot_potential</code>             \u2013              <p>Plot \\(U(x)\\) vs x.</p> </li> <li> <code>sample</code>             \u2013              <p>Simulate and return samples modulo 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>(ndarray, shape(n_samples, 1))</code> )          \u2013            <p>Final positions of particles samples.</p> </li> </ul>"},{"location":"reference/datasets/#fpsl.datasets.WPotential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the W-potential at x.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array_like, shape(1))</code>)           \u2013            <p>Position in [0,1].</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time (ignored for static potential).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>U</code> (              <code>float</code> )          \u2013            <p>Potential energy \\(U(x)\\).</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Evaluate the W-potential at x.\n\n    Parameters\n    ----------\n    x : array_like, shape (1,)\n        Position in [0,1].\n    t : float\n        Time (ignored for static potential).\n\n    Returns\n    -------\n    U : float\n        Potential energy $U(x)$.\n    \"\"\"\n    return w_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.WPotential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy W-Potential')</code>","text":"<p>Plot the potential energy curve.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>array_like</code>, default:                   <code>None</code> )           \u2013            <p>Grid points; defaults to self.x.</p> </li> <li> <code>title</code>               (<code>str</code>, default:                   <code>'Toy W-Potential'</code> )           \u2013            <p>Plot title.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ax</code> (              <code>Axes</code> )          \u2013            <p>Matplotlib Axes instance with the plot.</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy W-Potential',\n):\n    \"\"\"Plot the potential energy curve.\n\n    Parameters\n    ----------\n    x : array_like, optional\n        Grid points; defaults to self.x.\n    title : str, optional\n        Plot title.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Matplotlib Axes instance with the plot.\n    \"\"\"\n    if x is None:\n        x = self.x\n    vectorized_potential = jnp.vectorize(\n        lambda xv: self.potential(jnp.array([xv]), 0.0)\n    )\n    ax = plt.gca()\n    ax.plot(x, vectorized_potential(x), 'k--', label='ref')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$U(x)$')\n    ax.set_xlim(x[0], x[-1])\n    ax.set_title(title)\n    return ax\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.WPotential1D.sample","title":"<code>sample(key, dt=0.0001, n_steps=int(100000.0), n_samples=2048, beta=1.0)</code>","text":"<p>Sample positions via Euler-Maruyama integration.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>PRNG key.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Time step size.</p> </li> <li> <code>n_steps</code>               (<code>int</code>, default:                   <code>int(100000.0)</code> )           \u2013            <p>Number of heat-up steps.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>2048</code> )           \u2013            <p>Number of independent trajectories.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Inverse temperature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>(ndarray, shape(n_samples, 1))</code> )          \u2013            <p>Final positions modulo 1.</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    dt: Float[ArrayLike, ''] = 1e-4,\n    n_steps: Int[ArrayLike, ''] = int(1e5),\n    n_samples: Int[ArrayLike, ''] = 2048,\n    beta: Float[ArrayLike, ''] = 1.0,\n) -&gt; Float[ArrayLike, 'n_samples 1']:\n    \"\"\"Sample positions via Euler-Maruyama integration.\n\n    Parameters\n    ----------\n    key : JaxKey\n        PRNG key.\n    dt : float\n        Time step size.\n    n_steps : int\n        Number of heat-up steps.\n    n_samples : int\n        Number of independent trajectories.\n    beta : float\n        Inverse temperature.\n\n    Returns\n    -------\n    samples : ndarray, shape (n_samples, 1)\n        Final positions modulo 1.\n    \"\"\"\n    integrator = EulerMaruyamaIntegrator(\n        potential=self.potential,\n        n_dims=1,\n        dt=dt,\n        beta=beta,\n        n_heatup=n_steps,\n        gamma=self.gamma,\n    )\n    key1, key2 = jax.random.split(key)\n    trajs, _, _ = integrator.integrate(\n        key=key1,\n        X=jax.random.uniform(key2, (n_samples, 1)),\n        n_steps=0,\n    )\n    return trajs[-1] % 1\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembranePotential1D","title":"<code>ToyMembranePotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane\\_potential\\_1d}(x)\\). Inherits sampling and plotting behavior from WPotential1D.</p>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembranePotential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the toy membrane potential at x.\"\"\"\n    return toy_membrane_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembranePotential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane Potential')</code>","text":"<p>Plot the toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane Potential',\n):\n    \"\"\"Plot the toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane2Potential1D","title":"<code>ToyMembrane2Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a second toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane2\\_potential\\_1d}(x)\\).</p>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane2Potential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the second toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the second toy membrane potential at x.\"\"\"\n    return toy_membrane2_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane2Potential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane 2 Potential')</code>","text":"<p>Plot the second toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane 2 Potential',\n):\n    \"\"\"Plot the second toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane3Potential1D","title":"<code>ToyMembrane3Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a third toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane3\\_potential\\_1d}(x)\\).</p>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane3Potential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the third toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the third toy membrane potential at x.\"\"\"\n    return toy_membrane3_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/#fpsl.datasets.ToyMembrane3Potential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane 3 Potential')</code>","text":"<p>Plot the third toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane 3 Potential',\n):\n    \"\"\"Plot the third toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/datasets/","title":"datasets","text":"<p>Dataset classes for sampling from 1D toy potentials.</p> <p>This submodule provides interfaces and concrete implementations for sampling from various 1D potentials using (biased) Euler-Maruyama integrators.  Supported potentials include the W-potential and several \u201ctoy membrane\u201d variants.  Users can obtain sample trajectories or plot the potential energy profile.</p> <p>The following datasets are available:</p> <ul> <li>DataSet     Abstract base defining the dataset interface.</li> <li>WPotential1D     Dataset for the 1D W-potential.</li> <li>BiasedForceWPotential1D     WPotential1D with an additional bias force in sampling.</li> <li>ToyMembranePotential1D     Dataset for a 1D toy membrane potential.</li> <li>BiasedForceToyMembranePotential1D     Biased version of ToyMembranePotential1D.</li> <li>ToyMembrane2Potential1D     Dataset for a second toy membrane potential.</li> <li>BiasedForceToyMembrane2Potential1D     Biased version of ToyMembrane2Potential1D.</li> <li>ToyMembrane3Potential1D     Dataset for a third toy membrane potential.</li> <li>BiasedForceToyMembrane3Potential1D     Biased version of ToyMembrane3Potential1D.</li> </ul>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.DataSet","title":"<code>DataSet</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class defining dataset interface.</p> <p>All datasets must implement sampling and potential evaluation.</p> <p>Methods:</p> <ul> <li> <code>sample</code>             \u2013              <p>Draw samples from the target distribution.</p> </li> <li> <code>potential</code>             \u2013              <p>Evaluate the potential energy at position \\(x\\) and time \\(t\\).</p> </li> </ul>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.WPotential1D","title":"<code>WPotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataSet</code></p> <p>Dataset for the 1D W-potential.</p> <p>This class integrates the 1D W-potential \\(U(x)\\) using the overdamped Langevin dynamics (Euler-Maruyama).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array_like, shape(dim1))</code>, default:                   <code>linspace(0,1,100)</code> )           \u2013            <p>Grid points for plotting the potential.</p> </li> <li> <code>gamma</code>               (<code>callable</code>, default:                   <code>lambda x: 1.0</code> )           \u2013            <p>Friction function \\(\\gamma(x)\\), defaults to constant 1.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>potential</code>             \u2013              <p>Returns \\(U(x)\\) from w_potential_1d.</p> </li> <li> <code>plot_potential</code>             \u2013              <p>Plot \\(U(x)\\) vs x.</p> </li> <li> <code>sample</code>             \u2013              <p>Simulate and return samples modulo 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>(ndarray, shape(n_samples, 1))</code> )          \u2013            <p>Final positions of particles samples.</p> </li> </ul>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.WPotential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the W-potential at x.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array_like, shape(1))</code>)           \u2013            <p>Position in [0,1].</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time (ignored for static potential).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>U</code> (              <code>float</code> )          \u2013            <p>Potential energy \\(U(x)\\).</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Evaluate the W-potential at x.\n\n    Parameters\n    ----------\n    x : array_like, shape (1,)\n        Position in [0,1].\n    t : float\n        Time (ignored for static potential).\n\n    Returns\n    -------\n    U : float\n        Potential energy $U(x)$.\n    \"\"\"\n    return w_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.WPotential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy W-Potential')</code>","text":"<p>Plot the potential energy curve.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>array_like</code>, default:                   <code>None</code> )           \u2013            <p>Grid points; defaults to self.x.</p> </li> <li> <code>title</code>               (<code>str</code>, default:                   <code>'Toy W-Potential'</code> )           \u2013            <p>Plot title.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ax</code> (              <code>Axes</code> )          \u2013            <p>Matplotlib Axes instance with the plot.</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy W-Potential',\n):\n    \"\"\"Plot the potential energy curve.\n\n    Parameters\n    ----------\n    x : array_like, optional\n        Grid points; defaults to self.x.\n    title : str, optional\n        Plot title.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Matplotlib Axes instance with the plot.\n    \"\"\"\n    if x is None:\n        x = self.x\n    vectorized_potential = jnp.vectorize(\n        lambda xv: self.potential(jnp.array([xv]), 0.0)\n    )\n    ax = plt.gca()\n    ax.plot(x, vectorized_potential(x), 'k--', label='ref')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$U(x)$')\n    ax.set_xlim(x[0], x[-1])\n    ax.set_title(title)\n    return ax\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.WPotential1D.sample","title":"<code>sample(key, dt=0.0001, n_steps=int(100000.0), n_samples=2048, beta=1.0)</code>","text":"<p>Sample positions via Euler-Maruyama integration.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>PRNG key.</p> </li> <li> <code>dt</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Time step size.</p> </li> <li> <code>n_steps</code>               (<code>int</code>, default:                   <code>int(100000.0)</code> )           \u2013            <p>Number of heat-up steps.</p> </li> <li> <code>n_samples</code>               (<code>int</code>, default:                   <code>2048</code> )           \u2013            <p>Number of independent trajectories.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Inverse temperature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>(ndarray, shape(n_samples, 1))</code> )          \u2013            <p>Final positions modulo 1.</p> </li> </ul> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    dt: Float[ArrayLike, ''] = 1e-4,\n    n_steps: Int[ArrayLike, ''] = int(1e5),\n    n_samples: Int[ArrayLike, ''] = 2048,\n    beta: Float[ArrayLike, ''] = 1.0,\n) -&gt; Float[ArrayLike, 'n_samples 1']:\n    \"\"\"Sample positions via Euler-Maruyama integration.\n\n    Parameters\n    ----------\n    key : JaxKey\n        PRNG key.\n    dt : float\n        Time step size.\n    n_steps : int\n        Number of heat-up steps.\n    n_samples : int\n        Number of independent trajectories.\n    beta : float\n        Inverse temperature.\n\n    Returns\n    -------\n    samples : ndarray, shape (n_samples, 1)\n        Final positions modulo 1.\n    \"\"\"\n    integrator = EulerMaruyamaIntegrator(\n        potential=self.potential,\n        n_dims=1,\n        dt=dt,\n        beta=beta,\n        n_heatup=n_steps,\n        gamma=self.gamma,\n    )\n    key1, key2 = jax.random.split(key)\n    trajs, _, _ = integrator.integrate(\n        key=key1,\n        X=jax.random.uniform(key2, (n_samples, 1)),\n        n_steps=0,\n    )\n    return trajs[-1] % 1\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.BiasedForceWPotential1D","title":"<code>BiasedForceWPotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Biased-force dataset for the 1D W-potential.</p> <p>Extends WPotential1D by adding a bias force \\(b(x,t)\\).</p> <p>Parameters:</p> <ul> <li> <code>bias</code>               (<code>callable</code>)           \u2013            <p>Bias force function \\(b(x,t)\\).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>sample</code>             \u2013              <p>Simulate biased dynamics and return samples.</p> </li> </ul>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.BiasedForceWPotential1D.sample","title":"<code>sample(key, dt=0.0001, n_steps=int(100000.0), n_samples=2048, beta=1.0)</code>","text":"<p>Sample positions with bias force via Euler-Maruyama.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    dt: Float[ArrayLike, ''] = 1e-4,\n    n_steps: Int[ArrayLike, ''] = int(1e5),\n    n_samples: Int[ArrayLike, ''] = 2048,\n    beta: Float[ArrayLike, ''] = 1.0,\n) -&gt; Float[ArrayLike, 'n_samples 1']:\n    \"\"\"Sample positions with bias force via Euler-Maruyama.\"\"\"\n    integrator = BiasedForceEulerMaruyamaIntegrator(\n        bias_force=self.bias,\n        potential=self.potential,\n        n_dims=1,\n        dt=dt,\n        beta=beta,\n        n_heatup=n_steps,\n        gamma=self.gamma,\n    )\n    key1, key2 = jax.random.split(key)\n    trajs, _, _ = integrator.integrate(\n        key=key1,\n        X=jax.random.uniform(key2, (n_samples, 1)),\n        n_steps=0,\n    )\n    return trajs[-1] % 1\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembranePotential1D","title":"<code>ToyMembranePotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane\\_potential\\_1d}(x)\\). Inherits sampling and plotting behavior from WPotential1D.</p>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembranePotential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the toy membrane potential at x.\"\"\"\n    return toy_membrane_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembranePotential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane Potential')</code>","text":"<p>Plot the toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane Potential',\n):\n    \"\"\"Plot the toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.BiasedForceToyMembranePotential1D","title":"<code>BiasedForceToyMembranePotential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembranePotential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the toy membrane potential.</p>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane2Potential1D","title":"<code>ToyMembrane2Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a second toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane2\\_potential\\_1d}(x)\\).</p>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane2Potential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the second toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the second toy membrane potential at x.\"\"\"\n    return toy_membrane2_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane2Potential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane 2 Potential')</code>","text":"<p>Plot the second toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane 2 Potential',\n):\n    \"\"\"Plot the second toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.BiasedForceToyMembrane2Potential1D","title":"<code>BiasedForceToyMembrane2Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembrane2Potential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the second toy membrane potential.</p>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane3Potential1D","title":"<code>ToyMembrane3Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>WPotential1D</code></p> <p>Dataset for a third toy membrane potential in 1D.</p> <p>Overrides potential with \\(U(x)=\\mathrm{toy\\_membrane3\\_potential\\_1d}(x)\\).</p>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane3Potential1D.potential","title":"<code>potential(x, t)</code>","text":"<p>Evaluate the third toy membrane potential at x.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def potential(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ''],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Evaluate the third toy membrane potential at x.\"\"\"\n    return toy_membrane3_potential_1d(x[0])\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.ToyMembrane3Potential1D.plot_potential","title":"<code>plot_potential(x=None, title='Toy Membrane 3 Potential')</code>","text":"<p>Plot the third toy membrane potential curve.</p> Source code in <code>src/fpsl/datasets/datasets.py</code> <pre><code>def plot_potential(\n    self,\n    x: None | Float[ArrayLike, ' dim1'] = None,\n    title: str = 'Toy Membrane 3 Potential',\n):\n    \"\"\"Plot the third toy membrane potential curve.\"\"\"\n    return super().plot_potential(x, title)\n</code></pre>"},{"location":"reference/datasets/datasets/#fpsl.datasets.datasets.BiasedForceToyMembrane3Potential1D","title":"<code>BiasedForceToyMembrane3Potential1D(*, x=lambda: jnp.linspace(0, 1, 100)(), gamma=lambda x: 1.0, bias)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ToyMembrane3Potential1D</code>, <code>BiasedForceWPotential1D</code></p> <p>Biased-force dataset for the third toy membrane potential.</p>"},{"location":"reference/datasets/potentials/","title":"potentials","text":"<p>This module provides a collection of one-dimensional, periodic potential energy functions. Each function is JIT-compiled for efficiency and runtime type-checked using beartype.</p> <p>Each function accepts an array-like input <code>x</code> of arbitrary shape and returns a scalar or array of potential values corresponding to each element of <code>x</code>.</p> <p>Defined functions:</p> <ul> <li> <p>w_potential_1d(x):     Periodic double-well potential     \\(\\phi(x) = [2\\cos(2\\pi x) + 1]\\,\\cos(2\\pi x) - 0.628279.\\)</p> </li> <li> <p>toy_membrane_potential_1d(x):     Toy membrane potential     \\(\\phi(x) = -\\tfrac12\\Bigl(\\tfrac{(\\cos(4\\pi x)-1)^4}{4} + \\cos(2\\pi x)\\Bigr) + 0.862700.\\)</p> </li> <li> <p>toy_membrane2_potential_1d(x):     Parameterized periodic potential based on MD data     \\(\\phi(x) = \\phi_0 + \\sum_{i=1}^N \\alpha_i\\,\\cos(2\\pi i x),     \\quad \\phi_0 = 0.77830946.\\)</p> </li> <li> <p>toy_membrane3_potential_1d(x):     Alternative MD-based series potential     \\(\\phi(x) = \\phi_0 + \\sum_{i=1}^N \\alpha_i\\,\\cos(2\\pi i x),     \\quad \\phi_0 = 0.26733318.\\)</p> </li> </ul>"},{"location":"reference/datasets/potentials/#fpsl.datasets.potentials.w_potential_1d","title":"<code>w_potential_1d(x)</code>","text":"<p>Periodic double well potential.</p> \\[ \\phi(x) = [2\\cos(2\\pi x) + 1]\\cos(2\\pi x) \\] Source code in <code>src/fpsl/datasets/potentials.py</code> <pre><code>@jit\n@beartype\ndef w_potential_1d(x: Float[ArrayLike, '']) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Periodic double well potential.\n\n    $$\n    \\phi(x) = [2\\cos(2\\pi x) + 1]\\cos(2\\pi x)\n    $$\n    \"\"\"\n    return (2 * jnp.cos(2 * jnp.pi * x) ** 2 + jnp.cos(2 * jnp.pi * x) - 0.628279).sum()\n</code></pre>"},{"location":"reference/datasets/potentials/#fpsl.datasets.potentials.toy_membrane_potential_1d","title":"<code>toy_membrane_potential_1d(x)</code>","text":"<p>Periodic toy membrane potential.</p> \\[ \\phi(x) = - \\frac{1}{2} \\left[\\frac{(\\cos(4\\pi x) -1)^4}{4} +\\cos(2\\pi x)\\right] \\] Source code in <code>src/fpsl/datasets/potentials.py</code> <pre><code>@jit\n@beartype\ndef toy_membrane_potential_1d(x: Float[ArrayLike, '']) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Periodic toy membrane potential.\n\n    $$\n    \\phi(x) = - \\frac{1}{2} \\left[\\frac{(\\cos(4\\pi x) -1)^4}{4} +\\cos(2\\pi x)\\right]\n    $$\n    \"\"\"\n    return (\n        -1\n        / 2\n        * ((jnp.cos(4 * jnp.pi * x) - 1) ** 4 / 4 + jnp.cos(2 * jnp.pi * x)).sum()\n        + 0.862700\n    )\n</code></pre>"},{"location":"reference/datasets/potentials/#fpsl.datasets.potentials.toy_membrane2_potential_1d","title":"<code>toy_membrane2_potential_1d(x)</code>","text":"<p>Periodic toy membrane potential based on MD.</p> Source code in <code>src/fpsl/datasets/potentials.py</code> <pre><code>@jit\n@beartype\ndef toy_membrane2_potential_1d(x: Float[ArrayLike, '']) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Periodic toy membrane potential based on MD.\"\"\"\n    alphas = jnp.array([\n        4.38756764e-01,\n        5.51305175e-01,\n        -9.66697633e-01,\n        4.76220012e-01,\n        3.51388663e-01,\n        -6.57680452e-01,\n        3.20680887e-01,\n        1.19107859e-02,\n        4.46960330e-04,\n        -1.06695265e-01,\n        1.18188225e-02,\n        9.63761136e-02,\n        -6.34292215e-02,\n        -1.26926098e-02,\n        -2.20742077e-03,\n        3.11103798e-02,\n        -1.16342846e-02,\n        -2.06445083e-02,\n        1.19867604e-02,\n        2.14913208e-03,\n    ])\n    ns = jnp.arange(1, len(alphas) + 1)\n    phi_0 = 0.77830946\n    return phi_0 + (alphas * jnp.cos(2 * jnp.pi * x * ns)).sum()\n</code></pre>"},{"location":"reference/datasets/potentials/#fpsl.datasets.potentials.toy_membrane3_potential_1d","title":"<code>toy_membrane3_potential_1d(x)</code>","text":"<p>Periodic toy membrane potential based on MD.</p> Source code in <code>src/fpsl/datasets/potentials.py</code> <pre><code>@jit\n@beartype\ndef toy_membrane3_potential_1d(x: Float[ArrayLike, '']) -&gt; Float[ArrayLike, '']:\n    r\"\"\"Periodic toy membrane potential based on MD.\"\"\"\n    alphas = jnp.array([\n        -6.5813565e-01,\n        6.3420063e-01,\n        -5.1591349e-01,\n        1.2018956e-01,\n        2.8954777e-01,\n        -3.8310459e-01,\n        1.6142714e-01,\n        3.7272871e-02,\n        -4.2626891e-02,\n        -4.5450684e-03,\n        -6.4875120e-03,\n        3.0953294e-02,\n        -1.6555822e-02,\n        -7.3198415e-03,\n        9.5287291e-03,\n        -1.1982573e-03,\n        8.0206152e-04,\n        -1.3514888e-03,\n        1.1800211e-03,\n        2.3124311e-03,\n    ])\n    ns = jnp.arange(1, len(alphas) + 1)\n    phi_0 = 0.26733318\n    return phi_0 + (alphas * jnp.cos(2 * jnp.pi * x * ns)).sum()\n</code></pre>"},{"location":"reference/ddm/","title":"ddm","text":"<p>Denoising Diffusion Models (DDM) for score-based generative modeling.</p> <p>This submodule provides a comprehensive suite of components for building and training score-based denoising diffusion models, with a focus on periodic data and force-conditioned sampling.</p> <p>The submodule is structured into the following submodules:</p> <ul> <li>models:     Core FPSL (Fokker-Planck Score Learning) diffusion model implementation     for learning score functions and generating samples.</li> <li>network:     Neural network architectures including MLPs with Fourier feature embeddings     for score function approximation on periodic domains.</li> <li>noiseschedule:     Time-dependent noise scheduling functions that control the variance and     diffusion coefficients during the forward/reverse processes.</li> <li>prior:     Latent prior distribution definitions, including uniform priors with     periodic boundary conditions for circular/toroidal data.</li> <li>priorschedule:     Interpolation schedules between data and prior distributions that control     the mixing coefficient \\(\\alpha(t)\\) during diffusion.</li> <li>forceschedule:     Force conditioning schedules that control how external forces influence     the diffusion process through time-dependent scaling factors.</li> </ul>"},{"location":"reference/ddm/#fpsl.ddm.FPSL","title":"<code>FPSL(*, sigma_min=0.05, sigma_max=0.5, is_periodic=True, mlp_network, key, n_sample_steps=100, n_epochs=100, batch_size=128, wandb_log=False, gamma_energy_regulariztion=1e-05, fourier_features=1, warmup_steps=5, box_size=1.0, symmetric=False, diffusion=lambda x: 1.0, pbc_bins=0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LinearForceSchedule</code>, <code>LinearPriorSchedule</code>, <code>UniformPrior</code>, <code>ExponetialVarianceNoiseSchedule</code>, <code>DefaultDataClass</code></p> <p>Fokker-Planck Score Learning (FPSL) model for periodic data.</p> <p>An energy-based denoising diffusion model designed for learning probability distributions on periodic domains [0, 1]. The model combines multiple inheritance from various schedule and prior classes to provide a complete diffusion modeling framework with force scheduling capabilities.</p> <p>This implementation uses JAX for efficient computation and supports both symmetric and asymmetric periodic MLPs for score function approximation.</p> <p>Parameters:</p> <ul> <li> <code>mlp_network</code>               (<code>tuple[int]</code>)           \u2013            <p>Architecture of the MLP network as a tuple specifying the number of units in each hidden layer.</p> </li> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>JAX random key for reproducible random number generation.</p> </li> <li> <code>n_sample_steps</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of integration steps for the sampling process.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of training epochs.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Batch size for training.</p> </li> <li> <code>wandb_log</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log training metrics to Weights &amp; Biases.</p> </li> <li> <code>gamma_energy_regulariztion</code>               (<code>float</code>, default:                   <code>1e-5</code> )           \u2013            <p>Regularization coefficient for energy term in the loss function.</p> </li> <li> <code>fourier_features</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of Fourier features to use in the network.</p> </li> <li> <code>warmup_steps</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of warmup steps for learning rate scheduling.</p> </li> <li> <code>box_size</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Size of the periodic box domain. Currently, this is not used to scale the input data.</p> </li> <li> <code>symmetric</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use symmetric (cos-only) periodic MLP architecture or a periodic (sin+cos) MLP architecture.</p> </li> <li> <code>diffusion</code>               (<code>Callable[[Float[ArrayLike, ' n_features']], Float[ArrayLike, '']]</code>, default:                   <code>lambda x: 1.0</code> )           \u2013            <p>Position-dependent diffusion function. Defaults to constant diffusion.</p> </li> <li> <code>pbc_bins</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of bins for periodic boundary condition corrections. If 0, no PBC corrections are applied.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>params</code>               (<code>Params</code>)           \u2013            <p>Trained model parameters (available after training).</p> </li> <li> <code>dim</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data (set during training).</p> </li> <li> <code>score_model</code>               (<code>ScorePeriodicMLP or ScoreSymmetricPeriodicMLP</code>)           \u2013            <p>The neural network used for score function approximation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>train</code>             \u2013              <p>Train the model on provided data (\\(X\\in[0, 1]\\)) with forces.</p> </li> <li> <code>sample</code>             \u2013              <p>Generate samples from the learned distribution.</p> </li> <li> <code>evaluate</code>             \u2013              <p>Evaluate the model loss on held-out data.</p> </li> <li> <code>score</code>             \u2013              <p>Compute the score function at given positions and time.</p> </li> <li> <code>energy</code>             \u2013              <p>Compute the energy function at given positions and time.</p> </li> </ul> Notes <p>The model implements the Fokker-Planck score learning approach for diffusion models on periodic domains. It combines:</p> <ul> <li>Linear force scheduling for non-equilibrium dynamics</li> <li>Linear prior scheduling for interpolation between prior and data</li> <li>Exponential variance noise scheduling</li> <li>Uniform prior distribution on [0, 1]</li> </ul> <p>The training objective includes both score matching and energy regularization terms, with support for periodic boundary conditions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; from fpsl.ddm.models import FPSL\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; key = jr.PRNGKey(42)\n&gt;&gt;&gt; model = FPSL(\n...     mlp_network=(64, 64, 64),\n...     key=key,\n...     n_epochs=50,\n...     batch_size=64\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Train on data\n&gt;&gt;&gt; X = jr.uniform(key, (1000, 1))  # periodic data\n&gt;&gt;&gt; y = jr.normal(key, (1000, 1))   # force data\n&gt;&gt;&gt; lrs = [1e-6, 1e-4]  # Learning rate range\n&gt;&gt;&gt; loss_hist = model.train(X, y, lrs)\n</code></pre>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.score_model","title":"<code>score_model</code>  <code>cached</code> <code>property</code>","text":"<p>Create and cache the neural network.</p>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.score","title":"<code>score(x, t, y=None)</code>","text":"<p>Compute the diffusion score function at given positions and time.</p> <p>The score function represents the gradient of the log probability density with respect to the input coordinates: \\(\\nabla_x \\ln p_t(x)\\).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Input positions where to evaluate the score function.</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time parameter in [0, 1], where t=1 is pure noise and t=0 is data.</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ''] or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional force/conditioning variable. If None, uses equilibrium score.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, 'n_samples n_features']</code>           \u2013            <p>Score function values at each input position.</p> </li> </ul> Notes <p>The score function is computed as:</p> \\[     s_\\theta(x, t) = \\nabla_x \\ln p_t(x) = -\\frac{\\nabla_x E_\\theta(x, t)}{\\sigma(t)} \\] Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def score(\n    self,\n    x: Float[ArrayLike, 'n_samples n_features'],\n    t: float,\n    y: None | Float[ArrayLike, ''] = None,\n) -&gt; Float[ArrayLike, 'n_samples n_features']:\n    r\"\"\"Compute the diffusion score function at given positions and time.\n\n    The score function represents the gradient of the log probability density\n    with respect to the input coordinates: $\\nabla_x \\ln p_t(x)$.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, 'n_samples n_features']\n        Input positions where to evaluate the score function.\n    t : float\n        Time parameter in [0, 1], where t=1 is pure noise and t=0 is data.\n    y : Float[ArrayLike, ''] or None, default=None\n        Optional force/conditioning variable. If None, uses equilibrium score.\n\n    Returns\n    -------\n    Float[ArrayLike, 'n_samples n_features']\n        Score function values at each input position.\n\n    Notes\n    -----\n    The score function is computed as:\n\n    $$\n        s_\\theta(x, t) = \\nabla_x \\ln p_t(x) = -\\frac{\\nabla_x E_\\theta(x, t)}{\\sigma(t)}\n    $$\n    \"\"\"\n    if self.sigma(t) == 0:  # catch division by zero\n        return np.zeros_like(x)\n\n    score_times_minus_sigma = jax.vmap(\n        self._score_eq, in_axes=(None, 0, 0),\n    )(self.params, x, jnp.full((len(x), 1), t)) if y is None else jax.vmap(\n        self._score, in_axes=(None, 0, 0, 0),\n    )(self.params, x, jnp.full((len(x), 1), t), jnp.full((len(x), 1), y))  # fmt: skip\n\n    return -score_times_minus_sigma / self.sigma(t)\n</code></pre>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.energy","title":"<code>energy(x, t, y=None)</code>","text":"<p>Compute the energy function at given positions and time.</p> <p>The energy function represents the negative log probability density up to a constant: \\(E_\\theta(x, t) = -\\ln p_t(x) + C\\).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Input positions where to evaluate the energy function.</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time parameter in [0, 1], where \\(t=1\\) is pure noise and \\(t=0\\) is data.</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ''] or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional force/conditioning variable. If None, uses equilibrium energy.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, ' n_samples']</code>           \u2013            <p>Energy function values at each input position.</p> </li> </ul> Notes <p>The energy function is related to the score function by: $$     \\nabla_x E_\\theta(x, t) = -s_\\theta(x, t)\\sigma(t) $$</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def energy(\n    self,\n    x: Float[ArrayLike, 'n_samples n_features'],\n    t: float,\n    y: None | Float[ArrayLike, ''] = None,\n) -&gt; Float[ArrayLike, ' n_samples']:\n    r\"\"\"Compute the energy function at given positions and time.\n\n    The energy function represents the negative log probability density\n    up to a constant: $E_\\theta(x, t) = -\\ln p_t(x) + C$.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, 'n_samples n_features']\n        Input positions where to evaluate the energy function.\n    t : float\n        Time parameter in [0, 1], where $t=1$ is pure noise and $t=0$ is data.\n    y : Float[ArrayLike, ''] or None, default=None\n        Optional force/conditioning variable. If None, uses equilibrium energy.\n\n    Returns\n    -------\n    Float[ArrayLike, ' n_samples']\n        Energy function values at each input position.\n\n    Notes\n    -----\n    The energy function is related to the score function by:\n    $$\n        \\nabla_x E_\\theta(x, t) = -s_\\theta(x, t)\\sigma(t)\n    $$\n    \"\"\"\n    # catch division by zero\n    if isinstance(t, float) and self.sigma(t) == 0:\n        return np.zeros_like(x)\n\n    energy_times_minus_sigma = (\n        jax.vmap(\n            self._energy_eq,\n            in_axes=(None, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t))\n        if y is None\n        else jax.vmap(\n            self._energy,\n            in_axes=(None, 0, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t), jnp.full((len(x), 1), y))\n    )\n\n    return -energy_times_minus_sigma / self.sigma(t) + jax.vmap(\n        self._ln_diffusion_t,\n    )(x, jnp.full((len(x), 1), t))\n</code></pre>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.train","title":"<code>train(X, y, lrs, key=None, n_epochs=None, X_val=None, y_val=None, project='entropy-prod-diffusion', wandb_kwargs={})</code>","text":"<p>Train the FPSL model on the provided dataset.</p> <p>This method trains the score function neural network using a combination of score matching loss and energy regularization. The training uses warmup cosine decay learning rate scheduling and AdamW optimizer.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Training data positions. Must be 2D array with shape (n_samples, n_features). Data should be in the periodic domain [0, 1].</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ' n_features']</code>)           \u2013            <p>Force/conditioning variables corresponding to each sample in X.</p> </li> <li> <code>lrs</code>               (<code>Float[ArrayLike, 2]</code>)           \u2013            <p>Learning rate range as [min_lr, max_lr] for warmup cosine decay schedule.</p> </li> <li> <code>key</code>               (<code>JaxKey or None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for reproducible training. If None, uses self.key.</p> </li> <li> <code>n_epochs</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Number of training epochs. If None, uses self.n_epochs.</p> </li> <li> <code>X_val</code>               (<code>Float[ArrayLike, 'n_val n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Validation data positions. If provided, validation loss will be computed.</p> </li> <li> <code>y_val</code>               (<code>Float[ArrayLike, ' n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Validation force variables. Required if X_val is provided.</p> </li> <li> <code>project</code>               (<code>str</code>, default:                   <code>'entropy-prod-diffusion'</code> )           \u2013            <p>Weights &amp; Biases project name for logging (if wandb_log=True).</p> </li> <li> <code>wandb_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to wandb.init().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing training history with keys: - 'train_loss': Array of training losses for each epoch - 'val_loss': Array of validation losses (if validation data provided)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If X is not a 2D array.</p> </li> </ul> Notes <p>The training objective combines: 1. Score matching loss with periodic boundary handling 2. Energy regularization term controlled by <code>gamma_energy_regularization</code></p> <p>The model parameters are stored in <code>self.params</code> after training.</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def train(\n    self,\n    X: Float[ArrayLike, 'n_samples n_features'],\n    y: Float[ArrayLike, ' n_features'],\n    lrs: Float[ArrayLike, '2'],\n    key: None | JaxKey = None,\n    n_epochs: None | int = None,\n    X_val: None | Float[ArrayLike, 'n_val n_features'] = None,\n    y_val: None | Float[ArrayLike, ' n_features'] = None,\n    project: str = 'entropy-prod-diffusion',\n    wandb_kwargs: dict = {},\n):\n    \"\"\"Train the FPSL model on the provided dataset.\n\n    This method trains the score function neural network using a combination\n    of score matching loss and energy regularization. The training uses\n    warmup cosine decay learning rate scheduling and AdamW optimizer.\n\n    Parameters\n    ----------\n    X : Float[ArrayLike, 'n_samples n_features']\n        Training data positions. Must be 2D array with shape (n_samples, n_features).\n        Data should be in the periodic domain [0, 1].\n    y : Float[ArrayLike, ' n_features']\n        Force/conditioning variables corresponding to each sample in X.\n    lrs : Float[ArrayLike, '2']\n        Learning rate range as [min_lr, max_lr] for warmup cosine decay schedule.\n    key : JaxKey or None, default=None\n        Random key for reproducible training. If None, uses self.key.\n    n_epochs : int or None, default=None\n        Number of training epochs. If None, uses self.n_epochs.\n    X_val : Float[ArrayLike, 'n_val n_features'] or None, default=None\n        Validation data positions. If provided, validation loss will be computed.\n    y_val : Float[ArrayLike, ' n_features'] or None, default=None\n        Validation force variables. Required if X_val is provided.\n    project : str, default='entropy-prod-diffusion'\n        Weights &amp; Biases project name for logging (if wandb_log=True).\n    wandb_kwargs : dict, default={}\n        Additional keyword arguments passed to wandb.init().\n\n    Returns\n    -------\n    dict\n        Dictionary containing training history with keys:\n        - 'train_loss': Array of training losses for each epoch\n        - 'val_loss': Array of validation losses (if validation data provided)\n\n    Raises\n    ------\n    ValueError\n        If X is not a 2D array.\n\n    Notes\n    -----\n    The training objective combines:\n    1. Score matching loss with periodic boundary handling\n    2. Energy regularization term controlled by `gamma_energy_regularization`\n\n    The model parameters are stored in `self.params` after training.\n    \"\"\"\n    if X.ndim == 1:\n        raise ValueError('X must be 2D array.')\n\n    # if self.wandb_log and dataset is None:\n    #    raise ValueError('Please provide a dataset for logging.')\n\n    if key is None:\n        key = self.key\n\n    if n_epochs is None:\n        n_epochs = self.n_epochs\n\n    self.dim: int = X.shape[-1]\n\n    # start a new wandb run to track this script\n    if self.wandb_log:\n        wandb.init(\n            project=project,\n            config=self._get_config(\n                lrs=lrs,\n                key=key,\n                n_epochs=n_epochs,\n                X=X,\n                y=y,\n            )\n            | wandb_kwargs,\n        )\n\n    # main logic\n    loss_hist = self._train(\n        X=X,\n        lrs=lrs,\n        key=key,\n        n_epochs=n_epochs,\n        y=y,\n        X_val=X_val,\n        y_val=y_val,\n    )\n\n    if self.wandb_log:\n        wandb.finish()\n    return loss_hist\n</code></pre>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.evaluate","title":"<code>evaluate(X, y=None, key=None)</code>","text":"<p>Evaluate the model loss on held-out data.</p> <p>Computes the same loss function used during training (score matching + energy regularization) on the provided data without updating model parameters.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Test data positions in the periodic domain [0, 1].</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ' n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Force/conditioning variables for the test data. If None, assumes equilibrium evaluation.</p> </li> <li> <code>key</code>               (<code>JaxKey or None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for stochastic evaluation. If None, uses self.key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Evaluation loss value.</p> </li> </ul> Notes <p>This method is useful for monitoring generalization performance on validation or test sets during or after training.</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def evaluate(\n    self,\n    X: Float[ArrayLike, 'n_samples n_features'],\n    y: None | Float[ArrayLike, ' n_features'] = None,\n    key: None | JaxKey = None,\n) -&gt; float:\n    \"\"\"Evaluate the model loss on held-out data.\n\n    Computes the same loss function used during training (score matching\n    + energy regularization) on the provided data without updating model\n    parameters.\n\n    Parameters\n    ----------\n    X : Float[ArrayLike, 'n_samples n_features']\n        Test data positions in the periodic domain [0, 1].\n    y : Float[ArrayLike, ' n_features'] or None, default=None\n        Force/conditioning variables for the test data. If None, assumes\n        equilibrium evaluation.\n    key : JaxKey or None, default=None\n        Random key for stochastic evaluation. If None, uses self.key.\n\n    Returns\n    -------\n    float\n        Evaluation loss value.\n\n    Notes\n    -----\n    This method is useful for monitoring generalization performance on\n    validation or test sets during or after training.\n    \"\"\"\n    if key is None:\n        key = self.key\n    loss_fn = self._create_loss_fn()\n    return float(loss_fn(params=self.params, key=key, X=X, y=y))\n</code></pre>"},{"location":"reference/ddm/#fpsl.ddm.FPSL.sample","title":"<code>sample(key, n_samples, t_final=0, n_steps=None)</code>","text":"<p>Generate samples from the learned probability distribution.</p> <p>Uses reverse-time SDE integration to generate samples by starting from the prior distribution and integrating backwards through the diffusion process using the learned score function.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>Random key for reproducible sampling.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>Number of samples to generate.</p> </li> <li> <code>t_final</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Final time for the reverse integration. \\(t=0\\) corresponds to the data distribution, \\(t=1\\) to pure noise.</p> </li> <li> <code>n_steps</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Number of integration steps for the reverse SDE. If None, uses self.n_sample_steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, 'n_samples n_dims']</code>           \u2013            <p>Generated samples from the learned distribution.</p> </li> </ul> Notes <p>The sampling procedure follows the reverse-time SDE:</p> \\[     \\mathrm{d}x = [\\beta(t) s_\\theta(x, t)] \\mathrm{d}t + \\sqrt{\\beta(t)}\\mathrm{d}W \\] <p>where \\(s_\\theta\\) is the learned score function and \\(\\beta(t)\\) is the noise schedule. For periodic domains, the samples are wrapped to \\([0, 1]\\) at each step.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Generate 100 samples\n&gt;&gt;&gt; samples = model.sample(key, n_samples=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate with custom integration steps\n&gt;&gt;&gt; samples = model.sample(key, n_samples=50, n_steps=200)\n</code></pre> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    n_samples: int,\n    t_final: float = 0,\n    n_steps: None | int = None,\n) -&gt; Float[ArrayLike, 'n_samples n_dims']:\n    r\"\"\"Generate samples from the learned probability distribution.\n\n    Uses reverse-time SDE integration to generate samples by starting\n    from the prior distribution and integrating backwards through the\n    diffusion process using the learned score function.\n\n    Parameters\n    ----------\n    key : JaxKey\n        Random key for reproducible sampling.\n    n_samples : int\n        Number of samples to generate.\n    t_final : float, default=0\n        Final time for the reverse integration. $t=0$ corresponds to the\n        data distribution, $t=1$ to pure noise.\n    n_steps : int or None, default=None\n        Number of integration steps for the reverse SDE. If None, uses\n        self.n_sample_steps.\n\n    Returns\n    -------\n    Float[ArrayLike, 'n_samples n_dims']\n        Generated samples from the learned distribution.\n\n    Notes\n    -----\n    The sampling procedure follows the reverse-time SDE:\n\n    $$\n        \\mathrm{d}x = [\\beta(t) s_\\theta(x, t)] \\mathrm{d}t + \\sqrt{\\beta(t)}\\mathrm{d}W\n    $$\n\n    where $s_\\theta$ is the learned score function and $\\beta(t)$ is the noise schedule.\n    For periodic domains, the samples are wrapped to $[0, 1]$ at each step.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Generate 100 samples\n    &gt;&gt;&gt; samples = model.sample(key, n_samples=100)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Generate with custom integration steps\n    &gt;&gt;&gt; samples = model.sample(key, n_samples=50, n_steps=200)\n    \"\"\"\n    x_init = self.prior_sample(key, (n_samples, self.dim))\n    if n_steps is None:\n        n_steps = self.n_sample_steps\n    dt = (1 - t_final) / n_steps\n    t_array = jnp.linspace(1, t_final, n_steps + 1)\n\n    def body_fn(i, val):\n        x, key = val\n        key, subkey = jax.random.split(key)\n        t_curr = t_array[i]\n        eps = jax.random.normal(subkey, x.shape)\n\n        score_times_minus_sigma = jax.vmap(\n            self._score_eq,\n            in_axes=(None, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t_curr))\n        score = -score_times_minus_sigma / self.sigma(t_curr)\n\n        x_new = (\n            x\n            + self.beta(t_curr) * score * dt\n            + jnp.sqrt(self.beta(t_curr)) * eps * jnp.sqrt(dt)\n        )\n        if self.is_periodic:\n            x_new = x_new % 1\n        return (x_new, key)\n\n    final_x, _ = jax.lax.fori_loop(\n        0,\n        n_steps + 1,\n        body_fn,\n        (x_init, key),\n    )\n    return final_x\n</code></pre>"},{"location":"reference/ddm/forceschedule/","title":"forceschedule","text":"<p>Force schedule functions for driven DDPM diffusion models.</p> <p>This module defines the base abstract class for force schedules and two concrete implementations that map a normalized time variable \\(t \\in [0,1]\\) to a force scaling coefficient \\(\\alpha_{\\mathrm{force}}(t)\\).</p> <p>Classes:</p> <ul> <li> <code>ForceSchedule</code>           \u2013            <p>Abstract base class for force schedules.</p> </li> <li> <code>LinearForceSchedule</code>           \u2013            <p>Fades in force linearly: \\(\\alpha_{\\mathrm{force}}(t) = 1 - t\\).</p> </li> <li> <code>ConstantForceSchedule</code>           \u2013            <p>Constant force: \\(\\alpha_{\\mathrm{force}}(t) = 1\\).</p> </li> </ul>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.ForceSchedule","title":"<code>ForceSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for force schedules.</p> <p>Defines the interface for the time-dependent force scaling coefficient \\(\\alpha_{\\mathrm{force}}(t)\\) in driven DDPM models.</p> <p>Methods:</p> <ul> <li> <code>alpha_force</code>             \u2013              <p>Compute force scaling at time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.ForceSchedule.alpha_force","title":"<code>alpha_force(t)</code>  <code>abstractmethod</code>","text":"<p>Compute the force scaling coefficient at time \\(t\\).</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps, where \\(t \\in [0, 1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Force scaling coefficients.</p> </li> </ul> Source code in <code>src/fpsl/ddm/forceschedule.py</code> <pre><code>@abstractmethod\ndef alpha_force(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute the force scaling coefficient at time $t$.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps, where $t \\in [0, 1]$.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Force scaling coefficients.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.LinearForceSchedule","title":"<code>LinearForceSchedule</code>","text":"<p>               Bases: <code>ForceSchedule</code></p> <p>Linearly fading force schedule.</p> <p>Implements:</p> \\[ \\alpha_{\\mathrm{force}}(t) = 1 - t \\] <p>Methods:</p> <ul> <li> <code>alpha_force</code>             \u2013              <p>Returns \\(1 - t\\) for each time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.LinearForceSchedule.alpha_force","title":"<code>alpha_force(t)</code>","text":"<p>Compute linearly fading force scaling.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Force scaling coefficients equal to \\(1 - t\\).</p> </li> </ul> Source code in <code>src/fpsl/ddm/forceschedule.py</code> <pre><code>def alpha_force(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute linearly fading force scaling.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Force scaling coefficients equal to $1 - t$.\n    \"\"\"\n    return 1 - t\n</code></pre>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.ConstantForceSchedule","title":"<code>ConstantForceSchedule</code>","text":"<p>               Bases: <code>ForceSchedule</code></p> <p>Constant force schedule.</p> <p>Implements:</p> \\[ \\alpha_{\\mathrm{force}}(t) = 1 \\] <p>Methods:</p> <ul> <li> <code>alpha_force</code>             \u2013              <p>Returns 1 for each time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/forceschedule/#fpsl.ddm.forceschedule.ConstantForceSchedule.alpha_force","title":"<code>alpha_force(t)</code>","text":"<p>Compute constant force scaling.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Force scaling coefficients all equal to 1.</p> </li> </ul> Source code in <code>src/fpsl/ddm/forceschedule.py</code> <pre><code>def alpha_force(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute constant force scaling.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Force scaling coefficients all equal to 1.\n    \"\"\"\n    return jnp.ones_like(t)\n</code></pre>"},{"location":"reference/ddm/models/","title":"models","text":""},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL","title":"<code>FPSL(*, sigma_min=0.05, sigma_max=0.5, is_periodic=True, mlp_network, key, n_sample_steps=100, n_epochs=100, batch_size=128, wandb_log=False, gamma_energy_regulariztion=1e-05, fourier_features=1, warmup_steps=5, box_size=1.0, symmetric=False, diffusion=lambda x: 1.0, pbc_bins=0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LinearForceSchedule</code>, <code>LinearPriorSchedule</code>, <code>UniformPrior</code>, <code>ExponetialVarianceNoiseSchedule</code>, <code>DefaultDataClass</code></p> <p>Fokker-Planck Score Learning (FPSL) model for periodic data.</p> <p>An energy-based denoising diffusion model designed for learning probability distributions on periodic domains [0, 1]. The model combines multiple inheritance from various schedule and prior classes to provide a complete diffusion modeling framework with force scheduling capabilities.</p> <p>This implementation uses JAX for efficient computation and supports both symmetric and asymmetric periodic MLPs for score function approximation.</p> <p>Parameters:</p> <ul> <li> <code>mlp_network</code>               (<code>tuple[int]</code>)           \u2013            <p>Architecture of the MLP network as a tuple specifying the number of units in each hidden layer.</p> </li> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>JAX random key for reproducible random number generation.</p> </li> <li> <code>n_sample_steps</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of integration steps for the sampling process.</p> </li> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of training epochs.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>128</code> )           \u2013            <p>Batch size for training.</p> </li> <li> <code>wandb_log</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log training metrics to Weights &amp; Biases.</p> </li> <li> <code>gamma_energy_regulariztion</code>               (<code>float</code>, default:                   <code>1e-5</code> )           \u2013            <p>Regularization coefficient for energy term in the loss function.</p> </li> <li> <code>fourier_features</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of Fourier features to use in the network.</p> </li> <li> <code>warmup_steps</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of warmup steps for learning rate scheduling.</p> </li> <li> <code>box_size</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Size of the periodic box domain. Currently, this is not used to scale the input data.</p> </li> <li> <code>symmetric</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use symmetric (cos-only) periodic MLP architecture or a periodic (sin+cos) MLP architecture.</p> </li> <li> <code>diffusion</code>               (<code>Callable[[Float[ArrayLike, ' n_features']], Float[ArrayLike, '']]</code>, default:                   <code>lambda x: 1.0</code> )           \u2013            <p>Position-dependent diffusion function. Defaults to constant diffusion.</p> </li> <li> <code>pbc_bins</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of bins for periodic boundary condition corrections. If 0, no PBC corrections are applied.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>params</code>               (<code>Params</code>)           \u2013            <p>Trained model parameters (available after training).</p> </li> <li> <code>dim</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the data (set during training).</p> </li> <li> <code>score_model</code>               (<code>ScorePeriodicMLP or ScoreSymmetricPeriodicMLP</code>)           \u2013            <p>The neural network used for score function approximation.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>train</code>             \u2013              <p>Train the model on provided data (\\(X\\in[0, 1]\\)) with forces.</p> </li> <li> <code>sample</code>             \u2013              <p>Generate samples from the learned distribution.</p> </li> <li> <code>evaluate</code>             \u2013              <p>Evaluate the model loss on held-out data.</p> </li> <li> <code>score</code>             \u2013              <p>Compute the score function at given positions and time.</p> </li> <li> <code>energy</code>             \u2013              <p>Compute the energy function at given positions and time.</p> </li> </ul> Notes <p>The model implements the Fokker-Planck score learning approach for diffusion models on periodic domains. It combines:</p> <ul> <li>Linear force scheduling for non-equilibrium dynamics</li> <li>Linear prior scheduling for interpolation between prior and data</li> <li>Exponential variance noise scheduling</li> <li>Uniform prior distribution on [0, 1]</li> </ul> <p>The training objective includes both score matching and energy regularization terms, with support for periodic boundary conditions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.random as jr\n&gt;&gt;&gt; from fpsl.ddm.models import FPSL\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; key = jr.PRNGKey(42)\n&gt;&gt;&gt; model = FPSL(\n...     mlp_network=(64, 64, 64),\n...     key=key,\n...     n_epochs=50,\n...     batch_size=64\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Train on data\n&gt;&gt;&gt; X = jr.uniform(key, (1000, 1))  # periodic data\n&gt;&gt;&gt; y = jr.normal(key, (1000, 1))   # force data\n&gt;&gt;&gt; lrs = [1e-6, 1e-4]  # Learning rate range\n&gt;&gt;&gt; loss_hist = model.train(X, y, lrs)\n</code></pre>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.score_model","title":"<code>score_model</code>  <code>cached</code> <code>property</code>","text":"<p>Create and cache the neural network.</p>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.score","title":"<code>score(x, t, y=None)</code>","text":"<p>Compute the diffusion score function at given positions and time.</p> <p>The score function represents the gradient of the log probability density with respect to the input coordinates: \\(\\nabla_x \\ln p_t(x)\\).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Input positions where to evaluate the score function.</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time parameter in [0, 1], where t=1 is pure noise and t=0 is data.</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ''] or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional force/conditioning variable. If None, uses equilibrium score.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, 'n_samples n_features']</code>           \u2013            <p>Score function values at each input position.</p> </li> </ul> Notes <p>The score function is computed as:</p> \\[     s_\\theta(x, t) = \\nabla_x \\ln p_t(x) = -\\frac{\\nabla_x E_\\theta(x, t)}{\\sigma(t)} \\] Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def score(\n    self,\n    x: Float[ArrayLike, 'n_samples n_features'],\n    t: float,\n    y: None | Float[ArrayLike, ''] = None,\n) -&gt; Float[ArrayLike, 'n_samples n_features']:\n    r\"\"\"Compute the diffusion score function at given positions and time.\n\n    The score function represents the gradient of the log probability density\n    with respect to the input coordinates: $\\nabla_x \\ln p_t(x)$.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, 'n_samples n_features']\n        Input positions where to evaluate the score function.\n    t : float\n        Time parameter in [0, 1], where t=1 is pure noise and t=0 is data.\n    y : Float[ArrayLike, ''] or None, default=None\n        Optional force/conditioning variable. If None, uses equilibrium score.\n\n    Returns\n    -------\n    Float[ArrayLike, 'n_samples n_features']\n        Score function values at each input position.\n\n    Notes\n    -----\n    The score function is computed as:\n\n    $$\n        s_\\theta(x, t) = \\nabla_x \\ln p_t(x) = -\\frac{\\nabla_x E_\\theta(x, t)}{\\sigma(t)}\n    $$\n    \"\"\"\n    if self.sigma(t) == 0:  # catch division by zero\n        return np.zeros_like(x)\n\n    score_times_minus_sigma = jax.vmap(\n        self._score_eq, in_axes=(None, 0, 0),\n    )(self.params, x, jnp.full((len(x), 1), t)) if y is None else jax.vmap(\n        self._score, in_axes=(None, 0, 0, 0),\n    )(self.params, x, jnp.full((len(x), 1), t), jnp.full((len(x), 1), y))  # fmt: skip\n\n    return -score_times_minus_sigma / self.sigma(t)\n</code></pre>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.energy","title":"<code>energy(x, t, y=None)</code>","text":"<p>Compute the energy function at given positions and time.</p> <p>The energy function represents the negative log probability density up to a constant: \\(E_\\theta(x, t) = -\\ln p_t(x) + C\\).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Input positions where to evaluate the energy function.</p> </li> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Time parameter in [0, 1], where \\(t=1\\) is pure noise and \\(t=0\\) is data.</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ''] or None</code>, default:                   <code>None</code> )           \u2013            <p>Optional force/conditioning variable. If None, uses equilibrium energy.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, ' n_samples']</code>           \u2013            <p>Energy function values at each input position.</p> </li> </ul> Notes <p>The energy function is related to the score function by: $$     \\nabla_x E_\\theta(x, t) = -s_\\theta(x, t)\\sigma(t) $$</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def energy(\n    self,\n    x: Float[ArrayLike, 'n_samples n_features'],\n    t: float,\n    y: None | Float[ArrayLike, ''] = None,\n) -&gt; Float[ArrayLike, ' n_samples']:\n    r\"\"\"Compute the energy function at given positions and time.\n\n    The energy function represents the negative log probability density\n    up to a constant: $E_\\theta(x, t) = -\\ln p_t(x) + C$.\n\n    Parameters\n    ----------\n    x : Float[ArrayLike, 'n_samples n_features']\n        Input positions where to evaluate the energy function.\n    t : float\n        Time parameter in [0, 1], where $t=1$ is pure noise and $t=0$ is data.\n    y : Float[ArrayLike, ''] or None, default=None\n        Optional force/conditioning variable. If None, uses equilibrium energy.\n\n    Returns\n    -------\n    Float[ArrayLike, ' n_samples']\n        Energy function values at each input position.\n\n    Notes\n    -----\n    The energy function is related to the score function by:\n    $$\n        \\nabla_x E_\\theta(x, t) = -s_\\theta(x, t)\\sigma(t)\n    $$\n    \"\"\"\n    # catch division by zero\n    if isinstance(t, float) and self.sigma(t) == 0:\n        return np.zeros_like(x)\n\n    energy_times_minus_sigma = (\n        jax.vmap(\n            self._energy_eq,\n            in_axes=(None, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t))\n        if y is None\n        else jax.vmap(\n            self._energy,\n            in_axes=(None, 0, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t), jnp.full((len(x), 1), y))\n    )\n\n    return -energy_times_minus_sigma / self.sigma(t) + jax.vmap(\n        self._ln_diffusion_t,\n    )(x, jnp.full((len(x), 1), t))\n</code></pre>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.train","title":"<code>train(X, y, lrs, key=None, n_epochs=None, X_val=None, y_val=None, project='entropy-prod-diffusion', wandb_kwargs={})</code>","text":"<p>Train the FPSL model on the provided dataset.</p> <p>This method trains the score function neural network using a combination of score matching loss and energy regularization. The training uses warmup cosine decay learning rate scheduling and AdamW optimizer.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Training data positions. Must be 2D array with shape (n_samples, n_features). Data should be in the periodic domain [0, 1].</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ' n_features']</code>)           \u2013            <p>Force/conditioning variables corresponding to each sample in X.</p> </li> <li> <code>lrs</code>               (<code>Float[ArrayLike, 2]</code>)           \u2013            <p>Learning rate range as [min_lr, max_lr] for warmup cosine decay schedule.</p> </li> <li> <code>key</code>               (<code>JaxKey or None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for reproducible training. If None, uses self.key.</p> </li> <li> <code>n_epochs</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Number of training epochs. If None, uses self.n_epochs.</p> </li> <li> <code>X_val</code>               (<code>Float[ArrayLike, 'n_val n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Validation data positions. If provided, validation loss will be computed.</p> </li> <li> <code>y_val</code>               (<code>Float[ArrayLike, ' n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Validation force variables. Required if X_val is provided.</p> </li> <li> <code>project</code>               (<code>str</code>, default:                   <code>'entropy-prod-diffusion'</code> )           \u2013            <p>Weights &amp; Biases project name for logging (if wandb_log=True).</p> </li> <li> <code>wandb_kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to wandb.init().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing training history with keys: - 'train_loss': Array of training losses for each epoch - 'val_loss': Array of validation losses (if validation data provided)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If X is not a 2D array.</p> </li> </ul> Notes <p>The training objective combines: 1. Score matching loss with periodic boundary handling 2. Energy regularization term controlled by <code>gamma_energy_regularization</code></p> <p>The model parameters are stored in <code>self.params</code> after training.</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def train(\n    self,\n    X: Float[ArrayLike, 'n_samples n_features'],\n    y: Float[ArrayLike, ' n_features'],\n    lrs: Float[ArrayLike, '2'],\n    key: None | JaxKey = None,\n    n_epochs: None | int = None,\n    X_val: None | Float[ArrayLike, 'n_val n_features'] = None,\n    y_val: None | Float[ArrayLike, ' n_features'] = None,\n    project: str = 'entropy-prod-diffusion',\n    wandb_kwargs: dict = {},\n):\n    \"\"\"Train the FPSL model on the provided dataset.\n\n    This method trains the score function neural network using a combination\n    of score matching loss and energy regularization. The training uses\n    warmup cosine decay learning rate scheduling and AdamW optimizer.\n\n    Parameters\n    ----------\n    X : Float[ArrayLike, 'n_samples n_features']\n        Training data positions. Must be 2D array with shape (n_samples, n_features).\n        Data should be in the periodic domain [0, 1].\n    y : Float[ArrayLike, ' n_features']\n        Force/conditioning variables corresponding to each sample in X.\n    lrs : Float[ArrayLike, '2']\n        Learning rate range as [min_lr, max_lr] for warmup cosine decay schedule.\n    key : JaxKey or None, default=None\n        Random key for reproducible training. If None, uses self.key.\n    n_epochs : int or None, default=None\n        Number of training epochs. If None, uses self.n_epochs.\n    X_val : Float[ArrayLike, 'n_val n_features'] or None, default=None\n        Validation data positions. If provided, validation loss will be computed.\n    y_val : Float[ArrayLike, ' n_features'] or None, default=None\n        Validation force variables. Required if X_val is provided.\n    project : str, default='entropy-prod-diffusion'\n        Weights &amp; Biases project name for logging (if wandb_log=True).\n    wandb_kwargs : dict, default={}\n        Additional keyword arguments passed to wandb.init().\n\n    Returns\n    -------\n    dict\n        Dictionary containing training history with keys:\n        - 'train_loss': Array of training losses for each epoch\n        - 'val_loss': Array of validation losses (if validation data provided)\n\n    Raises\n    ------\n    ValueError\n        If X is not a 2D array.\n\n    Notes\n    -----\n    The training objective combines:\n    1. Score matching loss with periodic boundary handling\n    2. Energy regularization term controlled by `gamma_energy_regularization`\n\n    The model parameters are stored in `self.params` after training.\n    \"\"\"\n    if X.ndim == 1:\n        raise ValueError('X must be 2D array.')\n\n    # if self.wandb_log and dataset is None:\n    #    raise ValueError('Please provide a dataset for logging.')\n\n    if key is None:\n        key = self.key\n\n    if n_epochs is None:\n        n_epochs = self.n_epochs\n\n    self.dim: int = X.shape[-1]\n\n    # start a new wandb run to track this script\n    if self.wandb_log:\n        wandb.init(\n            project=project,\n            config=self._get_config(\n                lrs=lrs,\n                key=key,\n                n_epochs=n_epochs,\n                X=X,\n                y=y,\n            )\n            | wandb_kwargs,\n        )\n\n    # main logic\n    loss_hist = self._train(\n        X=X,\n        lrs=lrs,\n        key=key,\n        n_epochs=n_epochs,\n        y=y,\n        X_val=X_val,\n        y_val=y_val,\n    )\n\n    if self.wandb_log:\n        wandb.finish()\n    return loss_hist\n</code></pre>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.evaluate","title":"<code>evaluate(X, y=None, key=None)</code>","text":"<p>Evaluate the model loss on held-out data.</p> <p>Computes the same loss function used during training (score matching + energy regularization) on the provided data without updating model parameters.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Float[ArrayLike, 'n_samples n_features']</code>)           \u2013            <p>Test data positions in the periodic domain [0, 1].</p> </li> <li> <code>y</code>               (<code>Float[ArrayLike, ' n_features'] or None</code>, default:                   <code>None</code> )           \u2013            <p>Force/conditioning variables for the test data. If None, assumes equilibrium evaluation.</p> </li> <li> <code>key</code>               (<code>JaxKey or None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for stochastic evaluation. If None, uses self.key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Evaluation loss value.</p> </li> </ul> Notes <p>This method is useful for monitoring generalization performance on validation or test sets during or after training.</p> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def evaluate(\n    self,\n    X: Float[ArrayLike, 'n_samples n_features'],\n    y: None | Float[ArrayLike, ' n_features'] = None,\n    key: None | JaxKey = None,\n) -&gt; float:\n    \"\"\"Evaluate the model loss on held-out data.\n\n    Computes the same loss function used during training (score matching\n    + energy regularization) on the provided data without updating model\n    parameters.\n\n    Parameters\n    ----------\n    X : Float[ArrayLike, 'n_samples n_features']\n        Test data positions in the periodic domain [0, 1].\n    y : Float[ArrayLike, ' n_features'] or None, default=None\n        Force/conditioning variables for the test data. If None, assumes\n        equilibrium evaluation.\n    key : JaxKey or None, default=None\n        Random key for stochastic evaluation. If None, uses self.key.\n\n    Returns\n    -------\n    float\n        Evaluation loss value.\n\n    Notes\n    -----\n    This method is useful for monitoring generalization performance on\n    validation or test sets during or after training.\n    \"\"\"\n    if key is None:\n        key = self.key\n    loss_fn = self._create_loss_fn()\n    return float(loss_fn(params=self.params, key=key, X=X, y=y))\n</code></pre>"},{"location":"reference/ddm/models/#fpsl.ddm.models.FPSL.sample","title":"<code>sample(key, n_samples, t_final=0, n_steps=None)</code>","text":"<p>Generate samples from the learned probability distribution.</p> <p>Uses reverse-time SDE integration to generate samples by starting from the prior distribution and integrating backwards through the diffusion process using the learned score function.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>Random key for reproducible sampling.</p> </li> <li> <code>n_samples</code>               (<code>int</code>)           \u2013            <p>Number of samples to generate.</p> </li> <li> <code>t_final</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Final time for the reverse integration. \\(t=0\\) corresponds to the data distribution, \\(t=1\\) to pure noise.</p> </li> <li> <code>n_steps</code>               (<code>int or None</code>, default:                   <code>None</code> )           \u2013            <p>Number of integration steps for the reverse SDE. If None, uses self.n_sample_steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Float[ArrayLike, 'n_samples n_dims']</code>           \u2013            <p>Generated samples from the learned distribution.</p> </li> </ul> Notes <p>The sampling procedure follows the reverse-time SDE:</p> \\[     \\mathrm{d}x = [\\beta(t) s_\\theta(x, t)] \\mathrm{d}t + \\sqrt{\\beta(t)}\\mathrm{d}W \\] <p>where \\(s_\\theta\\) is the learned score function and \\(\\beta(t)\\) is the noise schedule. For periodic domains, the samples are wrapped to \\([0, 1]\\) at each step.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Generate 100 samples\n&gt;&gt;&gt; samples = model.sample(key, n_samples=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate with custom integration steps\n&gt;&gt;&gt; samples = model.sample(key, n_samples=50, n_steps=200)\n</code></pre> Source code in <code>src/fpsl/ddm/models.py</code> <pre><code>def sample(\n    self,\n    key: JaxKey,\n    n_samples: int,\n    t_final: float = 0,\n    n_steps: None | int = None,\n) -&gt; Float[ArrayLike, 'n_samples n_dims']:\n    r\"\"\"Generate samples from the learned probability distribution.\n\n    Uses reverse-time SDE integration to generate samples by starting\n    from the prior distribution and integrating backwards through the\n    diffusion process using the learned score function.\n\n    Parameters\n    ----------\n    key : JaxKey\n        Random key for reproducible sampling.\n    n_samples : int\n        Number of samples to generate.\n    t_final : float, default=0\n        Final time for the reverse integration. $t=0$ corresponds to the\n        data distribution, $t=1$ to pure noise.\n    n_steps : int or None, default=None\n        Number of integration steps for the reverse SDE. If None, uses\n        self.n_sample_steps.\n\n    Returns\n    -------\n    Float[ArrayLike, 'n_samples n_dims']\n        Generated samples from the learned distribution.\n\n    Notes\n    -----\n    The sampling procedure follows the reverse-time SDE:\n\n    $$\n        \\mathrm{d}x = [\\beta(t) s_\\theta(x, t)] \\mathrm{d}t + \\sqrt{\\beta(t)}\\mathrm{d}W\n    $$\n\n    where $s_\\theta$ is the learned score function and $\\beta(t)$ is the noise schedule.\n    For periodic domains, the samples are wrapped to $[0, 1]$ at each step.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Generate 100 samples\n    &gt;&gt;&gt; samples = model.sample(key, n_samples=100)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Generate with custom integration steps\n    &gt;&gt;&gt; samples = model.sample(key, n_samples=50, n_steps=200)\n    \"\"\"\n    x_init = self.prior_sample(key, (n_samples, self.dim))\n    if n_steps is None:\n        n_steps = self.n_sample_steps\n    dt = (1 - t_final) / n_steps\n    t_array = jnp.linspace(1, t_final, n_steps + 1)\n\n    def body_fn(i, val):\n        x, key = val\n        key, subkey = jax.random.split(key)\n        t_curr = t_array[i]\n        eps = jax.random.normal(subkey, x.shape)\n\n        score_times_minus_sigma = jax.vmap(\n            self._score_eq,\n            in_axes=(None, 0, 0),\n        )(self.params, x, jnp.full((len(x), 1), t_curr))\n        score = -score_times_minus_sigma / self.sigma(t_curr)\n\n        x_new = (\n            x\n            + self.beta(t_curr) * score * dt\n            + jnp.sqrt(self.beta(t_curr)) * eps * jnp.sqrt(dt)\n        )\n        if self.is_periodic:\n            x_new = x_new % 1\n        return (x_new, key)\n\n    final_x, _ = jax.lax.fori_loop(\n        0,\n        n_steps + 1,\n        body_fn,\n        (x_init, key),\n    )\n    return final_x\n</code></pre>"},{"location":"reference/ddm/network/","title":"network","text":"<p>Neural network architectures for score-based diffusion models.</p> <p>This module provides MLP-based score networks with optional Fourier feature embeddings for periodic domains.</p>"},{"location":"reference/ddm/network/#fpsl.ddm.network.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple feed-forward multilayer perceptron producing scalar outputs.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>Sequence[int]</code>)           \u2013            <p>Number of units in each hidden layer. A final Dense layer of size 1 is appended automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>ndarray</code> )          \u2013            <p>Array of shape (..., 1), the scalar output for each example.</p> </li> </ul>"},{"location":"reference/ddm/network/#fpsl.ddm.network.ScoreMLP","title":"<code>ScoreMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Time-dependent score network using a simple MLP.</p> <p>This network concatenates the state <code>x</code> and time embedding <code>t</code>, passes them through an MLP, and sums the final outputs to a scalar score.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>Sequence[int]</code>)           \u2013            <p>Hidden layer sizes for the underlying MLP.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (              <code>ndarray</code> )          \u2013            <p>Scalar score per example, shape (...,).</p> </li> </ul>"},{"location":"reference/ddm/network/#fpsl.ddm.network.ScorePeriodicMLP","title":"<code>ScorePeriodicMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Periodic score network with sine/cosine Fourier features.</p> <p>Extends <code>ScoreMLP</code> by first embedding <code>x</code> via Fourier features before concatenating with time.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>Sequence[int]</code>)           \u2013            <p>Hidden layer sizes for the underlying MLP.</p> </li> <li> <code>fourier_features_stop</code>               (<code>int</code>)           \u2013            <p>Maximum frequency (inclusive) for Fourier embeddings. Default is 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (              <code>ndarray</code> )          \u2013            <p>Scalar score per example, shape (...,).</p> </li> </ul>"},{"location":"reference/ddm/network/#fpsl.ddm.network.ScoreSymmetricPeriodicMLP","title":"<code>ScoreSymmetricPeriodicMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>Symmetric periodic score with cosine-only Fourier features.</p> <p>Similar to <code>ScorePeriodicMLP</code> but uses only cosine terms for even symmetry.</p> <p>Parameters:</p> <ul> <li> <code>features</code>               (<code>Sequence[int]</code>)           \u2013            <p>Hidden layer sizes for the underlying MLP.</p> </li> <li> <code>fourier_features_stop</code>               (<code>int</code>)           \u2013            <p>Maximum frequency (inclusive) for Fourier embeddings. Default is 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (              <code>ndarray</code> )          \u2013            <p>Scalar score per example, shape (...,).</p> </li> </ul>"},{"location":"reference/ddm/network/#fpsl.ddm.network.FourierFeatures","title":"<code>FourierFeatures</code>","text":"<p>               Bases: <code>Module</code></p> <p>Generate sine/cosine Fourier features for periodic inputs.</p> <p>Embeds each dimension of <code>x</code> into multiple frequencies via sine and/or cosine.</p> <p>Parameters:</p> <ul> <li> <code>start</code>               (<code>int</code>)           \u2013            <p>Starting frequency (inclusive). Default is 1.</p> </li> <li> <code>stop</code>               (<code>int</code>)           \u2013            <p>Stopping frequency (inclusive). Default is 8.</p> </li> <li> <code>step</code>               (<code>int</code>)           \u2013            <p>Frequency step size. Default is 1.</p> </li> <li> <code>odd</code>               (<code>bool</code>)           \u2013            <p>If True, include sine components. Default is True.</p> </li> <li> <code>even</code>               (<code>bool</code>)           \u2013            <p>If True, include cosine components. Default is True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If both <code>odd</code> and <code>even</code> are False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>features</code> (              <code>ndarray</code> )          \u2013            <p>Flattened array of shape (..., n_dims * n_freqs * n_components), where <code>n_components</code> is 1 or 2 depending on <code>odd</code>/<code>even</code>.</p> </li> </ul>"},{"location":"reference/ddm/noiseschedule/","title":"noiseschedule","text":"<p>Time-dependent noise schedules for DDPM diffusion models.</p> <p>This module defines an abstract base class for noise schedules and several concrete implementations for mapping a normalized time variable \\(t \\in [0,1]\\) to the noise parameters \\(\\beta(t)\\), \\(\\sigma(t)\\), and \\(\\gamma(t)\\):</p> <ul> <li>QuadraticVarianceNoiseSchedule:   \\(\\sigma(t)\\propto(\\sqrt{\\sigma_{\\min}/\\sigma_{\\max}}+t)^2\\).</li> <li>LinearVarianceNoiseSchedule:   \\(\\sigma(t)\\propto(\\sigma_{\\min}/\\sigma_{\\max}+t)\\).</li> <li>ExponentialVarianceNoiseSchedule:   \\(\\sigma(t)=\\sigma_{\\min}^{1-t}\\,\\sigma_{\\max}^t\\).</li> </ul>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.NoiseSchedule","title":"<code>NoiseSchedule(*, sigma_min, sigma_max)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DefaultDataClass</code>, <code>ABC</code></p> <p>Abstract base class for time-dependent noise schedules.</p> <p>Defines the noise schedule \\(\\beta(t)\\), the cummulative noise scale \\(\\sigma(t)\\), and the mean drift \\(\\gamma(t)\\) for a diffusion process given a normalized time \\(t \\in [0,1]\\).</p> <p>Attributes:</p> <ul> <li> <code>sigma_min</code>               (<code>float</code>)           \u2013            <p>Minimum noise scale at \\(t=0\\).</p> </li> <li> <code>sigma_max</code>               (<code>float</code>)           \u2013            <p>Maximum noise scale at \\(t=1\\).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>beta</code>             \u2013              <p>Instantaneous noise rate \\(\\beta(t)\\).</p> </li> <li> <code>sigma</code>             \u2013              <p>Noise scale \\(\\sigma(t)\\).</p> </li> <li> <code>gamma</code>             \u2013              <p>Mean drift \\(\\gamma(t)\\).</p> </li> </ul>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.NoiseSchedule.beta","title":"<code>beta(t)</code>  <code>abstractmethod</code>","text":"<p>Instantaneous noise rate \\(\\beta(t)\\).</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps \\(t \\in [0,1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>beta_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Noise rate at each time.</p> </li> </ul> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>@abstractmethod\ndef beta(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Instantaneous noise rate $\\beta(t)$.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps $t \\in [0,1]$.\n\n    Returns\n    -------\n    beta_t : array_like, shape (dim1,)\n        Noise rate at each time.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.NoiseSchedule.sigma","title":"<code>sigma(t)</code>  <code>abstractmethod</code>","text":"<p>Noise scale \\(\\sigma(t)\\).</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps \\(t \\in [0,1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>sigma_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Noise magnitude at each time.</p> </li> </ul> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>@abstractmethod\ndef sigma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Noise scale $\\sigma(t)$.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps $t \\in [0,1]$.\n\n    Returns\n    -------\n    sigma_t : array_like, shape (dim1,)\n        Noise magnitude at each time.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.NoiseSchedule.gamma","title":"<code>gamma(t)</code>  <code>abstractmethod</code>","text":"<p>Mean drift \\(\\gamma(t)\\).</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Normalized time steps \\(t \\in [0,1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>gamma_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Mean drift at each time.</p> </li> </ul> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>@abstractmethod\ndef gamma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Mean drift $\\gamma(t)$.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Normalized time steps $t \\in [0,1]$.\n\n    Returns\n    -------\n    gamma_t : array_like, shape (dim1,)\n        Mean drift at each time.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.QuadraticVarianceNoiseSchedule","title":"<code>QuadraticVarianceNoiseSchedule(*, sigma_min=0.07, sigma_max=0.5)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>NoiseSchedule</code></p> <p>Quadratic variance-exploding noise schedule.</p> <p>Defines:</p> \\[   \\sigma(t) = \\frac{(\\sqrt{\\sigma_{\\min}/\\sigma_{\\max}} + t)^2}                     {(\\sqrt{\\sigma_{\\min}/\\sigma_{\\max}} + 1)^2}                \\,\\sigma_{\\max},   \\quad \\beta(t) = \\frac{d}{dt}\\sigma(t)^2. \\] <p>Attributes:</p> <ul> <li> <code>sigma_min</code>               (<code>float, default=0.07</code>)           \u2013            <p>Starting noise scale.</p> </li> <li> <code>sigma_max</code>               (<code>float, default=0.5</code>)           \u2013            <p>Ending noise scale.</p> </li> </ul>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.QuadraticVarianceNoiseSchedule.gamma","title":"<code>gamma(t)</code>","text":"<p>No drift; not implemented.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def gamma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"No drift; not implemented.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.QuadraticVarianceNoiseSchedule.sigma","title":"<code>sigma(t)</code>","text":"<p>Compute \\(\\sigma(t)\\) as above.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def sigma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\sigma(t)$ as above.\"\"\"\n    factor = (jnp.sqrt(self.sigma_min / self.sigma_max) + t) ** 2\n    norm = (jnp.sqrt(self.sigma_min / self.sigma_max) + 1) ** 2\n    return factor / norm * self.sigma_max\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.QuadraticVarianceNoiseSchedule.beta","title":"<code>beta(t)</code>","text":"<p>Compute \\(\\beta(t) = \\frac{d}{dt}\\sigma(t)^2\\) via autograd.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def beta(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\beta(t) = \\frac{d}{dt}\\sigma(t)^2$ via autograd.\"\"\"\n    return jnp.vectorize(jax.grad(lambda tt: self.sigma(tt) ** 2))(t)\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.LinearVarianceNoiseSchedule","title":"<code>LinearVarianceNoiseSchedule(*, sigma_min=0.05, sigma_max=0.5)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>NoiseSchedule</code></p> <p>Linear variance\u2013exploding noise schedule.</p> <p>Defines:</p> \\[   \\sigma(t) = \\frac{(\\sigma_{\\min}/\\sigma_{\\max} + t)}                    {(\\sigma_{\\min}/\\sigma_{\\max} + 1)}                \\,\\sigma_{\\max},   \\quad \\beta(t) = \\frac{d}{dt}\\sigma(t)^2. \\]"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.LinearVarianceNoiseSchedule.gamma","title":"<code>gamma(t)</code>","text":"<p>No drift; not implemented.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def gamma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"No drift; not implemented.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.LinearVarianceNoiseSchedule.sigma","title":"<code>sigma(t)</code>","text":"<p>Compute \\(\\sigma(t)\\) as above.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def sigma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\sigma(t)$ as above.\"\"\"\n    factor = self.sigma_min / self.sigma_max + t\n    norm = self.sigma_min / self.sigma_max + 1\n    return factor / norm * self.sigma_max\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.LinearVarianceNoiseSchedule.beta","title":"<code>beta(t)</code>","text":"<p>Compute \\(\\beta(t) = \\frac{d}{dt}\\sigma(t)^2\\) via autograd.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def beta(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\beta(t) = \\frac{d}{dt}\\sigma(t)^2$ via autograd.\"\"\"\n    return jnp.vectorize(jax.grad(lambda tt: self.sigma(tt) ** 2))(t)\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.ExponetialVarianceNoiseSchedule","title":"<code>ExponetialVarianceNoiseSchedule(*, sigma_min=0.05, sigma_max=0.5)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>NoiseSchedule</code></p> <p>Exponential variance-exploding noise schedule.</p> <p>Defines:</p> \\[   \\sigma(t) = \\sigma_{\\min}^{1-t}\\,\\sigma_{\\max}^t,   \\quad \\beta(t) = \\frac{d}{dt}\\sigma(t)^2. \\]"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.ExponetialVarianceNoiseSchedule.gamma","title":"<code>gamma(t)</code>","text":"<p>No drift; not implemented.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def gamma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"No drift; not implemented.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.ExponetialVarianceNoiseSchedule.sigma","title":"<code>sigma(t)</code>","text":"<p>Compute \\(\\sigma(t) = \\sigma_{\\min}^{1-t}\\,\\sigma_{\\max}^t\\).</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def sigma(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\sigma(t) = \\sigma_{\\min}^{1-t}\\,\\sigma_{\\max}^t$.\"\"\"\n    return self.sigma_min ** (1 - t) * self.sigma_max**t\n</code></pre>"},{"location":"reference/ddm/noiseschedule/#fpsl.ddm.noiseschedule.ExponetialVarianceNoiseSchedule.beta","title":"<code>beta(t)</code>","text":"<p>Compute \\(\\beta(t) = \\frac{d}{dt}\\sigma(t)^2\\) via autograd.</p> Source code in <code>src/fpsl/ddm/noiseschedule.py</code> <pre><code>def beta(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute $\\beta(t) = \\frac{d}{dt}\\sigma(t)^2$ via autograd.\"\"\"\n    return jnp.vectorize(jax.grad(lambda tt: self.sigma(tt) ** 2))(t)\n</code></pre>"},{"location":"reference/ddm/prior/","title":"prior","text":"<p>Latent prior distributions for score-based diffusion models.</p> <p>This module defines the abstract base class for latent prior distributions and provides concrete implementations, such as the uniform prior on [0,1].</p> <p>Classes:</p> <ul> <li> <code>LatentPrior</code>           \u2013            <p>Abstract base class for latent priors.</p> </li> <li> <code>UniformPrior</code>           \u2013            <p>Uniform prior over [0,1] with periodic support.</p> </li> </ul>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.LatentPrior","title":"<code>LatentPrior(*, is_periodic=False)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DefaultDataClass</code>, <code>ABC</code></p> <p>Abstract base class for latent prior distributions.</p> <p>Attributes:</p> <ul> <li> <code>is_periodic</code>               (<code>bool</code>)           \u2013            <p>If True, the support of the prior is treated as periodic.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>_prior : str</code>             \u2013              <p>Name identifier of the prior (abstract property).</p> </li> <li> <code>prior_log_pdf</code>             \u2013              <p>Log probability density at x.</p> </li> <li> <code>prior_pdf</code>             \u2013              <p>Probability density at x.</p> </li> <li> <code>prior_sample</code>             \u2013              <p>Sample from the prior.</p> </li> <li> <code>prior_force</code>             \u2013              <p>Force term (gradient of log-pdf) at x.</p> </li> <li> <code>prior_x_t</code>             \u2013              <p>Diffuse x with noise and wrap if periodic.</p> </li> </ul>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior","title":"<code>UniformPrior(*, is_periodic=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LatentPrior</code></p> <p>Uniform prior over the unit interval [0, 1].</p> <p>A periodic prior with constant density and zero force.</p> <p>Attributes:</p> <ul> <li> <code>is_periodic</code>               (<code>bool</code>)           \u2013            <p>Always True for the uniform prior.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>prior_log_pdf</code>             \u2013              <p>Returns zero array (log-density).</p> </li> <li> <code>prior_pdf</code>             \u2013              <p>Returns one array (density).</p> </li> <li> <code>prior_sample</code>             \u2013              <p>Samples uniformly in [0,1].</p> </li> <li> <code>prior_force</code>             \u2013              <p>Returns zero array (gradient of log-density).</p> </li> <li> <code>prior_x_t</code>             \u2013              <p>Applies diffusion step with noise and wraps modulo 1.</p> </li> </ul>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior.prior_log_pdf","title":"<code>prior_log_pdf(x)</code>","text":"<p>Log-probability density: zero everywhere on [0,1].</p> Source code in <code>src/fpsl/ddm/prior.py</code> <pre><code>def prior_log_pdf(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Log-probability density: zero everywhere on [0,1].\"\"\"\n    return jnp.zeros_like(x)\n</code></pre>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior.prior_pdf","title":"<code>prior_pdf(x)</code>","text":"<p>Probability density: one everywhere on [0,1].</p> Source code in <code>src/fpsl/ddm/prior.py</code> <pre><code>def prior_pdf(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n) -&gt; Float[ArrayLike, '']:\n    \"\"\"Probability density: one everywhere on [0,1].\"\"\"\n    return jnp.ones_like(x)\n</code></pre>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior.prior_sample","title":"<code>prior_sample(key, shape)</code>","text":"<p>Sample uniformly from [0,1] with given JAX PRNG key.</p> Source code in <code>src/fpsl/ddm/prior.py</code> <pre><code>def prior_sample(\n    self,\n    key: JaxKey,\n    shape: tuple[int],\n) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"Sample uniformly from [0,1] with given JAX PRNG key.\"\"\"\n    return jax.random.uniform(key, shape)\n</code></pre>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior.prior_force","title":"<code>prior_force(x)</code>","text":"<p>Force term (gradient of log-pdf): zero everywhere.</p> Source code in <code>src/fpsl/ddm/prior.py</code> <pre><code>def prior_force(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"Force term (gradient of log-pdf): zero everywhere.\"\"\"\n    return jnp.zeros_like(x)\n</code></pre>"},{"location":"reference/ddm/prior/#fpsl.ddm.prior.UniformPrior.prior_x_t","title":"<code>prior_x_t(x, t, eps)</code>","text":"<p>Diffuse x with noise and wrap into [0,1].</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>(array - like, shape(dim1))</code>)           \u2013            <p>Current latent variable.</p> </li> <li> <code>t</code>               (<code>(array - like, shape(dim1))</code>)           \u2013            <p>Time embedding for noise scaling.</p> </li> <li> <code>eps</code>               (<code>(array - like, shape(dim1))</code>)           \u2013            <p>Standard normal noise sample.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_next</code> (              <code>(ndarray, shape(dim1))</code> )          \u2013            <p>Noisy update of x wrapped modulo 1.</p> </li> </ul> Source code in <code>src/fpsl/ddm/prior.py</code> <pre><code>def prior_x_t(\n    self,\n    x: Float[ArrayLike, ' dim1'],\n    t: Float[ArrayLike, ' dim1'],\n    eps: Float[ArrayLike, ' dim1'],\n) -&gt; Float[ArrayLike, ' dim1']:\n    \"\"\"Diffuse x with noise and wrap into [0,1].\n\n    Parameters\n    ----------\n    x : array-like, shape (dim1,)\n        Current latent variable.\n    t : array-like, shape (dim1,)\n        Time embedding for noise scaling.\n    eps : array-like, shape (dim1,)\n        Standard normal noise sample.\n\n    Returns\n    -------\n    x_next : jnp.ndarray, shape (dim1,)\n        Noisy update of x wrapped modulo 1.\n    \"\"\"\n    return (x + self.sigma(t) * eps) % 1\n</code></pre>"},{"location":"reference/ddm/priorschedule/","title":"priorschedule","text":"<p>Prior schedule functions for DDPM diffusion models.</p> <p>This module defines a base abstract class for prior schedules and two common concrete implementations:</p> <ul> <li>LinearPriorSchedule: \\(\\alpha(t) = t\\)</li> <li>QuadraticPriorSchedule: \\(\\alpha(t) = t^2\\)</li> </ul> <p>Classes:</p> <ul> <li> <code>PriorSchedule</code>           \u2013            <p>Abstract base for defining \\(\\alpha(t)\\) schedules.</p> </li> <li> <code>LinearPriorSchedule</code>           \u2013            <p>Simple linear schedule.</p> </li> <li> <code>QuadraticPriorSchedule</code>           \u2013            <p>Simple quadratic schedule.</p> </li> </ul> Notes <p>It is assumed that the time steps <code>t</code> are normalized to the range [0, 1].</p>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.PriorSchedule","title":"<code>PriorSchedule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for prior schedules in DDPM models.</p> <p>A prior schedule defines the mixing coefficient \\(\\alpha(t)\\) that controls how the model incorporates the prior distribution over time steps in a diffusion process.</p> <p>Methods:</p> <ul> <li> <code>alpha</code>             \u2013              <p>Compute the schedule coefficient \\(\\alpha\\) at time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.PriorSchedule.alpha","title":"<code>alpha(t)</code>  <code>abstractmethod</code>","text":"<p>Compute the schedule coefficient \\(\\alpha\\) at time \\(t\\).</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Time steps \\(t\\in[0, 1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Mixing coefficients corresponding to each time in \\(t\\).</p> </li> </ul> Source code in <code>src/fpsl/ddm/priorschedule.py</code> <pre><code>@abstractmethod\ndef alpha(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Compute the schedule coefficient $\\alpha$ at time $t$.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Time steps $t\\in[0, 1]$.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Mixing coefficients corresponding to each time in $t$.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.LinearPriorSchedule","title":"<code>LinearPriorSchedule</code>","text":"<p>               Bases: <code>PriorSchedule</code></p> <p>Linearly increasing prior schedule.</p> <p>\\(\\alpha(t) = t\\)</p> <p>Methods:</p> <ul> <li> <code>alpha</code>             \u2013              <p>Compute the schedule coefficient \\(\\alpha\\) at time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.LinearPriorSchedule.alpha","title":"<code>alpha(t)</code>","text":"<p>Linear schedule function.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Time steps \\(t\\in[0, 1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Equal to the input t.</p> </li> </ul> Source code in <code>src/fpsl/ddm/priorschedule.py</code> <pre><code>def alpha(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Linear schedule function.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Time steps $t\\in[0, 1]$.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Equal to the input t.\n    \"\"\"\n    return t\n</code></pre>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.QuadraticPriorSchedule","title":"<code>QuadraticPriorSchedule</code>","text":"<p>               Bases: <code>PriorSchedule</code></p> <p>Quadratically increasing prior schedule \\(\\alpha(t) = t^2\\).</p> <p>Methods:</p> <ul> <li> <code>alpha</code>             \u2013              <p>Compute the schedule coefficient \\(\\alpha\\) at time \\(t\\).</p> </li> </ul>"},{"location":"reference/ddm/priorschedule/#fpsl.ddm.priorschedule.QuadraticPriorSchedule.alpha","title":"<code>alpha(t)</code>","text":"<p>Quadratic schedule function.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>(array_like, shape(dim1))</code>)           \u2013            <p>Time steps \\(t\\in[0, 1]\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>alpha_t</code> (              <code>(array_like, shape(dim1))</code> )          \u2013            <p>Squares of the input t.</p> </li> </ul> Source code in <code>src/fpsl/ddm/priorschedule.py</code> <pre><code>def alpha(self, t: Float[ArrayLike, ' dim1']) -&gt; Float[ArrayLike, ' dim1']:\n    r\"\"\"Quadratic schedule function.\n\n    Parameters\n    ----------\n    t : array_like, shape (dim1,)\n        Time steps $t\\in[0, 1]$.\n\n    Returns\n    -------\n    alpha_t : array_like, shape (dim1,)\n        Squares of the input t.\n    \"\"\"\n    return t**2\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/baseclass/","title":"baseclass","text":""},{"location":"reference/utils/gmm/","title":"gmm","text":"<p>This submodule provides implementations of Gaussian Mixture Models (GMMs) with identical covariance across components, including a periodic extension to handle data on a bounded interval via component replication.</p> <p>Classes:</p> <ul> <li> <code>GMM</code>           \u2013            <p>Defines a mixture of N Gaussians with shared scalar standard deviation. Offers methods to compute the probability density function (PDF) and its natural logarithm over input samples.</p> </li> <li> <code>PeriodicGMM</code>           \u2013            <p>Inherits from GMM and adds support for periodic domains [0, bound]. It replicates mixture components across multiple copies of the domain to evaluate densities that respect periodic boundary conditions.</p> </li> </ul>"},{"location":"reference/utils/gmm/#fpsl.utils.gmm.GMM","title":"<code>GMM(means, std)</code>  <code>dataclass</code>","text":"<p>Gaussian Mixture Model of N Gaussians with identical covariance.</p> <p>Parameters:</p> <ul> <li> <code>means</code>               (<code>ndarray</code>)           \u2013            <p>The means of the Gaussians.</p> </li> <li> <code>std</code>               (<code>float</code>)           \u2013            <p>A scalar representing the standard deviation of the Gaussians.</p> </li> </ul>"},{"location":"reference/utils/gmm/#fpsl.utils.gmm.GMM.pdf","title":"<code>pdf(X)</code>","text":"<p>Calculate the probability density function (PDF) of the Gaussian Mixture Model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The PDF value.</p> </li> </ul> Source code in <code>src/fpsl/utils/gmm.py</code> <pre><code>def pdf(self, X: Float[ArrayLike, ' n_samples']) -&gt; Float[ArrayLike, ' n_samples']:\n    \"\"\"\n    Calculate the probability density function (PDF) of the Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    X : jnp.ndarray\n        The input data.\n\n    Returns\n    -------\n    float\n        The PDF value.\n\n    \"\"\"\n    return jax.scipy.stats.norm.pdf(\n        X,\n        loc=self.means,\n        scale=self.std,\n    ).mean(axis=0)\n</code></pre>"},{"location":"reference/utils/gmm/#fpsl.utils.gmm.GMM.ln_pdf","title":"<code>ln_pdf(X)</code>","text":"<p>Calculate the natural logarithm of the probability density function (PDF) of the Gaussian Mixture Model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The natural logarithm of the PDF value.</p> </li> </ul> Source code in <code>src/fpsl/utils/gmm.py</code> <pre><code>def ln_pdf(\n    self,\n    X: Float[ArrayLike, ' n_samples'],\n) -&gt; Float[ArrayLike, ' n_samples']:\n    \"\"\"\n    Calculate the natural logarithm of the probability density function (PDF) of the Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    X : jnp.ndarray\n        The input data.\n\n    Returns\n    -------\n    jnp.ndarray\n        The natural logarithm of the PDF value.\n\n    \"\"\"\n    return jnp.log(self.pdf(X))\n</code></pre>"},{"location":"reference/utils/gmm/#fpsl.utils.gmm.PeriodicGMM","title":"<code>PeriodicGMM(means, std, bound=1.0, copies=5)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>GMM</code>, <code>DefaultDataClass</code></p> <p>Gaussian Mixture Model of N Gaussians with identical covariance.</p> <p>Parameters:</p> <ul> <li> <code>means</code>               (<code>ndarray</code>)           \u2013            <p>The means of the Gaussians.</p> </li> <li> <code>std</code>               (<code>Union[ndarray, int]</code>)           \u2013            <p>A scalar representing the standard deviation of the Gaussians.</p> </li> <li> <code>bound</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>The data is periodic on [0, bound].</p> </li> <li> <code>copies</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of copies in each direction.</p> </li> </ul>"},{"location":"reference/utils/gmm/#fpsl.utils.gmm.PeriodicGMM.pdf","title":"<code>pdf(X)</code>","text":"<p>Calculate the probability density function (PDF) of the Gaussian Mixture Model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The input data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The PDF values.</p> </li> </ul> Source code in <code>src/fpsl/utils/gmm.py</code> <pre><code>def pdf(self, X: Float[ArrayLike, ' n_samples']) -&gt; Float[ArrayLike, ' n_samples']:\n    \"\"\"\n    Calculate the probability density function (PDF) of the Gaussian Mixture Model.\n\n    Parameters\n    ----------\n    X : jnp.ndarray\n        The input data.\n\n    Returns\n    -------\n    jnp.ndarray\n        The PDF values.\n\n    \"\"\"\n    return jnp.array([\n        jax.scipy.stats.norm.pdf(\n            X + offset,\n            loc=self.means,\n            scale=self.std,\n        ).mean(axis=0)\n        for offset in self.offsets\n    ]).sum(axis=0)\n</code></pre>"},{"location":"reference/utils/integrators/","title":"integrators","text":""},{"location":"reference/utils/integrators/#fpsl.utils.integrators.BrownianIntegrator","title":"<code>BrownianIntegrator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Overdamped Langevin eq. integrator.</p> <p>Solving the following SDE</p> \\[ \\mathrm{d}x = -\\phi(x, t)\\mathrm{d}t + \\sqrt{2\\beta^{-1}}\\mathrm{d}W_t \\]"},{"location":"reference/utils/integrators/#fpsl.utils.integrators.EulerMaruyamaIntegrator","title":"<code>EulerMaruyamaIntegrator(*, potential, n_dims, dt, beta, n_heatup=1000, gamma=lambda x: 1.0)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DefaultDataClass</code>, <code>BrownianIntegrator</code></p> <p>Euler\u2013Maruyama integrator for Langevin dynamics.</p> <p>This class implements the Euler\u2013Maruyama numerical scheme to integrate the overdamped Langevin SDE:</p> \\[ \\mathrm{d}x = -\\frac{\\nabla \\phi(x, t)}{\\gamma(x)} \\,\\mathrm{d}t + \\sqrt{\\frac{2}{\\beta\\,\\gamma(x)}} \\,\\mathrm{d}W_t \\] <p>where \\(\\phi(x, t)\\) is the potential energy, \\(\\gamma(x)\\) is the position-dependent friction coefficient, \\(\\beta\\) is the inverse temperature, and \\(W_t\\) is a standard Wiener process.</p> <p>Parameters:</p> <ul> <li> <code>potential</code>               (<code>Callable[[ArrayLike, float], ArrayLike]</code>)           \u2013            <p>Potential energy function \\(\\phi(x, t)\\). Accepts <code>x</code> of shape <code>(n_dims,)</code> and scalar <code>t</code>, returns scalar or array of shape <code>()</code>.</p> </li> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the state space.</p> </li> <li> <code>dt</code>               (<code>float</code>)           \u2013            <p>Time step size \\(\\Delta t\\).</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature parameter \\(\\beta = 1/(k_B T)\\).</p> </li> <li> <code>n_heatup</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of initial \u201cheat-up\u201d steps before recording trajectories. Default is 1000.</p> </li> <li> <code>gamma</code>               (<code>Callable[[ArrayLike], ArrayLike]</code>, default:                   <code>lambda x: 1.0</code> )           \u2013            <p>Position-dependent friction coefficient function \\(\\gamma(x)\\). Accepts <code>x</code> of shape <code>(n_dims,)</code>, returns scalar or array of shape <code>()</code>. Default is constant 1.0.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>potential</code>               (<code>Callable</code>)           \u2013            <p>(Scalar) Potential function \\(\\phi(x, t)\\).</p> </li> <li> <code>n_dims</code>               (<code>int</code>)           \u2013            <p>Dimensionality of the system.</p> </li> <li> <code>dt</code>               (<code>float</code>)           \u2013            <p>Integration time step \\(\\Delta t\\).</p> </li> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>Inverse temperature.</p> </li> <li> <code>n_heatup</code>               (<code>int</code>)           \u2013            <p>Number of pre-integration heat-up steps.</p> </li> <li> <code>gamma</code>               (<code>Callable</code>)           \u2013            <p>Position-dependent friction coefficient \\(\\gamma(x)\\).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>integrate</code>             \u2013              <p>Run the Euler\u2013Maruyama integrator over <code>n_steps</code>, returning: - <code>xs</code>: array of shape <code>(n_t_steps, n_samples, n_dims)</code> of positions, - <code>fs</code>: array of same shape for forces, - <code>ts</code>: time points of shape <code>(n_t_steps,)</code>.</p> </li> </ul>"},{"location":"reference/utils/integrators/#fpsl.utils.integrators.EulerMaruyamaIntegrator.integrate","title":"<code>integrate(key, X, n_steps=1000)</code>","text":"<p>Integrate Brownian dynamics using the Euler\u2013Maruyama scheme.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>PRNG key for JAX random number generation.</p> </li> <li> <code>X</code>               (<code>(array - like, shape(n_samples, n_dims))</code>)           \u2013            <p>Initial positions of the particles.</p> </li> <li> <code>n_steps</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of integration steps to perform. Default is 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>positions</code> (              <code>(ndarray, shape(n_t_steps, n_samples, n_dims))</code> )          \u2013            <p>Trajectories of the particles at each time step.</p> </li> <li> <code>forces</code> (              <code>(ndarray, shape(n_t_steps, n_samples, n_dims))</code> )          \u2013            <p>Deterministic forces \\(F = -\\nabla U(X, t)\\) evaluated along the trajectory.</p> </li> <li> <code>times</code> (              <code>(ndarray, shape(n_t_steps))</code> )          \u2013            <p>Time points corresponding to each integration step.</p> </li> </ul> Notes <p>The integrator approximates the overdamped Langevin equation</p> \\[     X_{t+1} = X_t + F(X_t, t)\\,\\Delta t + \\sqrt{2\\,\\Delta t}\\,\\xi_t, \\] <p>where:</p> <ul> <li>\\(F(X, t) = -\\nabla U(X, t)\\) is the conservative force,</li> <li>\\(\\Delta t\\) is the time step size (total time divided by \\(n\\) steps),</li> <li>\\(\\xi_t\\) are independent standard normal random variables.</li> </ul> Source code in <code>src/fpsl/utils/integrators.py</code> <pre><code>def integrate(\n    self,\n    key: JaxKey,\n    X: Float[ArrayLike, 'n_samples n_dims'],\n    n_steps: int = 1000,\n) -&gt; tuple[\n    Float[ArrayLike, 'n_t_steps n_samples n_dims'],\n    Float[ArrayLike, 'n_t_steps n_samples n_dims'],\n    Float[ArrayLike, ' n_t_steps'],\n]:\n    r\"\"\"\n    Integrate Brownian dynamics using the Euler\u2013Maruyama scheme.\n\n    Parameters\n    ----------\n    key : JaxKey\n        PRNG key for JAX random number generation.\n    X : array-like, shape (n_samples, n_dims)\n        Initial positions of the particles.\n    n_steps : int, optional\n        Number of integration steps to perform. Default is 1000.\n\n    Returns\n    -------\n    positions : ndarray, shape (n_t_steps, n_samples, n_dims)\n        Trajectories of the particles at each time step.\n    forces : ndarray, shape (n_t_steps, n_samples, n_dims)\n        Deterministic forces $F = -\\nabla U(X, t)$ evaluated along the trajectory.\n    times : ndarray, shape (n_t_steps,)\n        Time points corresponding to each integration step.\n\n    Notes\n    -----\n    The integrator approximates the overdamped Langevin equation\n\n    $$\n        X_{t+1} = X_t + F(X_t, t)\\,\\Delta t + \\sqrt{2\\,\\Delta t}\\,\\xi_t,\n    $$\n\n    where:\n\n    - $F(X, t) = -\\nabla U(X, t)$ is the conservative force,\n    - $\\Delta t$ is the time step size (total time divided by $n$ steps),\n    - $\\xi_t$ are independent standard normal random variables.\n    \"\"\"\n\n    @jax.jit\n    def force(X: Float[ArrayLike, 'n_samples n_dims'], t: float):\n        return -1 * jax.vmap(\n            jax.grad(self.potential, argnums=0),\n            in_axes=(0, None),\n        )(X, t)\n\n    return self._integrate(\n        key=key,\n        X=X,\n        n_steps=n_steps,\n        force=force,\n    )\n</code></pre>"},{"location":"reference/utils/integrators/#fpsl.utils.integrators.BiasedForceEulerMaruyamaIntegrator","title":"<code>BiasedForceEulerMaruyamaIntegrator(*, potential, n_dims, dt, beta, n_heatup=1000, gamma=lambda x: 1.0, bias_force=lambda x, t: np.zeros_like(x))</code>  <code>dataclass</code>","text":"<p>               Bases: <code>EulerMaruyamaIntegrator</code></p> <p>Euler\u2013Maruyama integrator with an additional bias force.</p> <p>This class extends EulerMaruyamaIntegrator by incorporating a user-specified bias force term into the overdamped Langevin SDE:</p> \\[ \\mathrm{d}x = -\\nabla \\phi(x, t)\\,\\mathrm{d}t + b(x, t)\\,\\mathrm{d}t + \\sqrt{\\frac{2}{\\beta\\,\\gamma(x)}}\\,\\mathrm{d}W_t, \\] <p>where   - \\(\\phi(x, t)\\) is the potential energy,   - \\(b(x, t)\\) is the bias force,   - \\(\\gamma(x)\\) is the (optional) position-dependent friction,   - \\(\\beta\\) is the inverse temperature,   - \\(W_t\\) is a standard Wiener process.</p> <p>Parameters:</p> <ul> <li> <code>bias_force</code>               (<code>Callable[[ArrayLike, float], ArrayLike]</code>, default:                   <code>lambda x, t: zeros_like(x)</code> )           \u2013            <p>User-defined bias force function \\(b(x, t)\\). Accepts <code>x</code> of shape <code>(n_dims,)</code> and scalar <code>t</code>, returns an array of shape <code>(n_dims,)</code>. Default is zero bias.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>All other keyword arguments are the same as for fpsl.utils.integrators.EulerMaruyamaIntegrator:</p> <ul> <li><code>potential</code>: potential energy function,</li> <li><code>n_dims</code>: number of dimensions,</li> <li><code>dt</code>: time step size,</li> <li><code>beta</code>: inverse temperature,</li> <li><code>n_heatup</code>: number of initial heat-up steps,</li> <li><code>gamma</code>: friction coefficient function.</li> </ul> </li> </ul>"},{"location":"reference/utils/integrators/#fpsl.utils.integrators.BiasedForceEulerMaruyamaIntegrator.integrate","title":"<code>integrate(key, X, n_steps=1000)</code>","text":"<p>Integrate Brownian dynamics using the Euler\u2013Maruyama scheme with bias.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>JaxKey</code>)           \u2013            <p>PRNG key for JAX random number generation.</p> </li> <li> <code>X</code>               (<code>(array - like, shape(n_samples, n_dims))</code>)           \u2013            <p>Initial positions of the particles.</p> </li> <li> <code>n_steps</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Number of integration steps to perform. Default is 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>positions</code> (              <code>(ndarray, shape(n_t_steps, n_samples, n_dims))</code> )          \u2013            <p>Trajectories of the particles at each time step, including heat-up.</p> </li> <li> <code>forces</code> (              <code>(ndarray, shape(n_t_steps, n_samples, n_dims))</code> )          \u2013            <p>Total forces \\(F = -\\nabla U(X,t) + b(X,t)\\) evaluated along the trajectory.</p> </li> <li> <code>times</code> (              <code>(ndarray, shape(n_t_steps))</code> )          \u2013            <p>Time points corresponding to each integration step.</p> </li> </ul> Source code in <code>src/fpsl/utils/integrators.py</code> <pre><code>def integrate(\n    self,\n    key: JaxKey,\n    X: Float[ArrayLike, 'n_samples n_dims'],\n    n_steps: int = 1000,\n) -&gt; tuple[\n    Float[ArrayLike, 'n_t_steps n_samples n_dims'],\n    Float[ArrayLike, 'n_t_steps n_samples n_dims'],\n    Float[ArrayLike, ' n_t_steps'],\n]:\n    r\"\"\"\n    Integrate Brownian dynamics using the Euler\u2013Maruyama scheme with bias.\n\n    Parameters\n    ----------\n    key : JaxKey\n        PRNG key for JAX random number generation.\n    X : array-like, shape (n_samples, n_dims)\n        Initial positions of the particles.\n    n_steps : int, optional\n        Number of integration steps to perform. Default is 1000.\n\n    Returns\n    -------\n    positions : ndarray, shape (n_t_steps, n_samples, n_dims)\n        Trajectories of the particles at each time step, including heat-up.\n    forces : ndarray, shape (n_t_steps, n_samples, n_dims)\n        Total forces $F = -\\nabla U(X,t) + b(X,t)$ evaluated along the trajectory.\n    times : ndarray, shape (n_t_steps,)\n        Time points corresponding to each integration step.\n\n    \"\"\"\n\n    @jax.jit\n    def force(X: Float[ArrayLike, 'n_samples n_dims'], t: float):\n        return -1 * jax.vmap(\n            jax.grad(self.potential, argnums=0),\n            in_axes=(0, None),\n        )(X, t) + jax.vmap(\n            self.bias_force,\n            in_axes=(0, None),\n        )(X, t)\n\n    return self._integrate(\n        key=key,\n        X=X,\n        n_steps=n_steps,\n        force=force,\n    )\n</code></pre>"},{"location":"reference/utils/typing/","title":"typing","text":""},{"location":"tutorials/","title":"Getting Started with <code>fpsl</code>","text":""},{"location":"tutorials/#introduction","title":"Introduction","text":"<p>If you are interested in learning free energy landscapes from non-equilibrium data using score-based diffusion models, this tutorial will provide you with a step-by-step guide to use <code>fpsl</code> (Fokker-Planck Score Learning), a Python package designed specifically for this purpose. FPSL employs the steady-state solution of the Fokker-Planck equation as an ansatz in the denoising score learning scheme, making it particularly effective for periodic boundary conditions and force-conditioned generation.</p> <p>Whether you are a beginner to score-based generative modeling or an experienced researcher in statistical mechanics, this tutorial will help you get up and running with FPSL and enable you to explore free energy landscapes of complex systems.</p>"},{"location":"tutorials/#installation","title":"Installation","text":"<p>Before you can use the FPSL package, you will need to install it on your machine. The package is available on PyPI, so you can install it using pip.</p> <p>To install the package using pip, open a terminal or command prompt and enter the following command:</p> <pre><code>python -m pip install fpsl\n</code></pre> <p>Alternatively, you can install the development version directly from GitHub:</p> <pre><code>python -m pip install git+https://github.com/BereauLab/fokker-planck-score-learning.git\n</code></pre> <p>Once you have installed the package, you are ready to explore the tutorial!</p>"},{"location":"tutorials/#tutorial","title":"Tutorial","text":"<p>The tutorial is structured into the following sections:</p> <ul> <li>Theoretical Background:     This section provides a theoretical background on the FPSL approach, including the Fokker-Planck equation, score-based generative modeling, and the specific challenges of periodic boundary conditions.</li> <li>Interactive Tutorial Notebook:     This comprehensive Jupyter notebook walks you through the complete FPSL workflow using toy model examples from the main paper. You'll learn how to: Set up and configure FPSL models, work with built-in potential energy datasets, train diffusion models on non-equilibrium data, generate samples and reconstruct free energy profiles, and analyze and visualize results.</li> </ul>"},{"location":"tutorials/#getting-help","title":"Getting Help","text":"<p>In case you encounter any issues or have questions while using FPSL, there are several resources available to help you:</p> <ul> <li>Paper: Read the original research article on arXiv:2506.15653</li> <li>Issues: Report bugs or request features on the GitHub repository</li> </ul>"},{"location":"tutorials/#conclusion","title":"Conclusion","text":"<p>FPSL represents a powerful approach to learning free energy landscapes from non-equilibrium data, particularly for systems with periodic boundary conditions. By the end of this tutorial, you will have a solid understanding of both the theoretical foundations and practical implementation of FPSL, enabling you to apply these techniques to your own research problems in statistical mechanics, molecular dynamics, and beyond.</p>"},{"location":"tutorials/advanced/","title":"Advanced Topics","text":"In\u00a0[1]: Copied! <pre>%env XLA_PYTHON_CLIENT_PREALLOCATE = false\n</pre> %env XLA_PYTHON_CLIENT_PREALLOCATE = false <pre>env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n</pre> In\u00a0[2]: Copied! <pre>from dataclasses import dataclass\nimport jax\nimport jax.numpy as jnp\n\nimport fpsl\nfrom fpsl import FPSL\n\n\n# define custom noise schedule\n@dataclass(kw_only=True)\nclass CustomNoiseSchedule(fpsl.ddm.noiseschedule.NoiseSchedule):\n    \"\"\"Custom noise schedule for FPSL.\"\"\"\n\n    sigma_min: float = 0.05\n    sigma_max: float = 0.5\n\n    @property\n    def _noise_schedule(self) -&gt; str:\n        return 'custom'  # this name is only used for logging\n\n    def gamma(self, t):\n        # this is for legacy reasons, as FPSL used to work also on non-periodic systems\n        raise NotImplementedError\n\n    def sigma(self, t):\n        # This is the default noise schedule used in FPSL.\n        # return self.sigma_min ** (1 - t) * self.sigma_max**t\n        return self.sigma_min ** (1 - t**2) * self.sigma_max ** (t**2)\n\n    def beta(self, t):\n        # Since we are too lazy to implement the analytical solution for the\n        # custom noise schedule, we use the numerical gradient of sigma.\n        return jnp.vectorize(jax.grad(lambda tt: self.sigma(tt) ** 2))(t)\n</pre> from dataclasses import dataclass import jax import jax.numpy as jnp  import fpsl from fpsl import FPSL   # define custom noise schedule @dataclass(kw_only=True) class CustomNoiseSchedule(fpsl.ddm.noiseschedule.NoiseSchedule):     \"\"\"Custom noise schedule for FPSL.\"\"\"      sigma_min: float = 0.05     sigma_max: float = 0.5      @property     def _noise_schedule(self) -&gt; str:         return 'custom'  # this name is only used for logging      def gamma(self, t):         # this is for legacy reasons, as FPSL used to work also on non-periodic systems         raise NotImplementedError      def sigma(self, t):         # This is the default noise schedule used in FPSL.         # return self.sigma_min ** (1 - t) * self.sigma_max**t         return self.sigma_min ** (1 - t**2) * self.sigma_max ** (t**2)      def beta(self, t):         # Since we are too lazy to implement the analytical solution for the         # custom noise schedule, we use the numerical gradient of sigma.         return jnp.vectorize(jax.grad(lambda tt: self.sigma(tt) ** 2))(t) <p>Now we can define the custom FPSL by:</p> In\u00a0[3]: Copied! <pre>@dataclass(kw_only=True)\nclass CustomFPSL(CustomNoiseSchedule, FPSL):\n    \"\"\"Custom FPSL class with custom noise schedule.\"\"\"\n\n    pass\n\n\n# generating an instance we find that is uses the new custom noise schedule\nCustomFPSL(\n    mlp_network=[32, 32, 32],\n    key=jax.random.PRNGKey(0),\n)._noise_schedule\n</pre> @dataclass(kw_only=True) class CustomFPSL(CustomNoiseSchedule, FPSL):     \"\"\"Custom FPSL class with custom noise schedule.\"\"\"      pass   # generating an instance we find that is uses the new custom noise schedule CustomFPSL(     mlp_network=[32, 32, 32],     key=jax.random.PRNGKey(0), )._noise_schedule Out[3]: <pre>'custom'</pre> <p>The same way, it is possible to change the force schedule, the prior sampling, the prior schedule, etc.</p>"},{"location":"tutorials/advanced/#extending-fokker-planck-score-learning-fpsl","title":"Extending Fokker-Planck Score Learning (FPSL)\u00b6","text":""},{"location":"tutorials/advanced/#note","title":"Note\u00b6","text":"<p>This tutorial is WIP. If your questions are not answered, please open an issue on GitHub.</p>"},{"location":"tutorials/advanced/#change-noise-schedule","title":"Change Noise Schedule\u00b6","text":"<p>To change the noise schedule, you can simply create your own class which inherits from a self-defined noise schedule.</p>"},{"location":"tutorials/fpsl/","title":"Example","text":"In\u00a0[1]: Copied! <pre>%env XLA_PYTHON_CLIENT_PREALLOCATE = false\n</pre> %env XLA_PYTHON_CLIENT_PREALLOCATE = false <pre>env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n</pre> In\u00a0[2]: Copied! <pre>import jax\nimport numpy as np\nimport prettypyplot as pplt\nimport jax_dataloader as jdl\nfrom jax import numpy as jnp\nfrom matplotlib import pyplot as plt\nfrom matplotlib import colors\n\nfrom fpsl.datasets.datasets import (\n    WPotential1D,\n    BiasedForceWPotential1D,\n)\nfrom fpsl import FPSL\n\n# fix random seeds\nkey = jax.random.key(42)\njdl.manual_seed(42)\nnp.random.seed(42)\n\n\n# ddpm training\nn_epochs = 50\nmlp_network = [96, 96, 96, 96]\nbatch_size = 2056\nlrs = (5e-3, 5e-7)\nsigma_min = 0.01\nsigma_max = 0.5\nfourier_feat = 4\nn_samples = batch_size * 512\n\n\n# setup plotting style\npplt.use_style(\n    colors='paula',\n    figsize=(1, 1),\n    cmap='plasma',\n    true_black=True,\n)\nplt.rcParams['figure.dpi'] = 200\ncnorm = colors.SymLogNorm(\n    linthresh=0.1,\n    linscale=1,\n    vmin=-20,\n    vmax=20,\n    base=10,\n)\n</pre> import jax import numpy as np import prettypyplot as pplt import jax_dataloader as jdl from jax import numpy as jnp from matplotlib import pyplot as plt from matplotlib import colors  from fpsl.datasets.datasets import (     WPotential1D,     BiasedForceWPotential1D, ) from fpsl import FPSL  # fix random seeds key = jax.random.key(42) jdl.manual_seed(42) np.random.seed(42)   # ddpm training n_epochs = 50 mlp_network = [96, 96, 96, 96] batch_size = 2056 lrs = (5e-3, 5e-7) sigma_min = 0.01 sigma_max = 0.5 fourier_feat = 4 n_samples = batch_size * 512   # setup plotting style pplt.use_style(     colors='paula',     figsize=(1, 1),     cmap='plasma',     true_black=True, ) plt.rcParams['figure.dpi'] = 200 cnorm = colors.SymLogNorm(     linthresh=0.1,     linscale=1,     vmin=-20,     vmax=20,     base=10, ) <p>Since we are training an MLP in this notebook it is important to use an GPU. The following will print how jax is initialized. If it shows <code>CPU</code>, but a GPU is available, you should install this package with <code>uv sync --extra cuda</code> or <code>pip install fpsl[cuda]</code>.</p> In\u00a0[3]: Copied! <pre>jax.devices()\n</pre> jax.devices() Out[3]: <pre>[CudaDevice(id=0)]</pre> In\u00a0[4]: Copied! <pre>def generate_data(ext_forces, key, pot):\n    \"\"\"Generate toy model data for training.\"\"\"\n    Xs = {}\n    for ext_force in ext_forces:\n        key, _ = jax.random.split(key)\n        bwpot = pot(\n            bias=lambda x, t: ext_force * jnp.ones_like(x),\n        )\n        samples = bwpot.sample(\n            key=key,\n            n_samples=n_samples,\n        )\n        Xs[int(ext_force)] = samples\n\n    return Xs, key\n\n\n# we pick here 3 different external forces to show how different simulations can be used to learn the same potential\next_forces = np.geomspace(3, 30, 4).astype(int)\n\nXs, key = generate_data(\n    ext_forces=ext_forces,\n    key=key,\n    pot=BiasedForceWPotential1D,\n)\n</pre> def generate_data(ext_forces, key, pot):     \"\"\"Generate toy model data for training.\"\"\"     Xs = {}     for ext_force in ext_forces:         key, _ = jax.random.split(key)         bwpot = pot(             bias=lambda x, t: ext_force * jnp.ones_like(x),         )         samples = bwpot.sample(             key=key,             n_samples=n_samples,         )         Xs[int(ext_force)] = samples      return Xs, key   # we pick here 3 different external forces to show how different simulations can be used to learn the same potential ext_forces = np.geomspace(3, 30, 4).astype(int)  Xs, key = generate_data(     ext_forces=ext_forces,     key=key,     pot=BiasedForceWPotential1D, ) <p>Now let us plot the sampled data.</p> In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots()\n\n# plot reference potential\nWPotential1D().plot_potential()\n\nbins = jnp.linspace(0, 1, 32)\nfor ext_force in ext_forces:\n    hist, _ = np.histogram(\n        Xs[ext_force],\n        bins=bins,\n        density=True,\n    )\n\n    ax.plot(\n        0.5 * (bins[1:] + bins[:-1]),\n        -np.log(hist),\n        label=ext_force,\n    )\n\nax.set_xlim(0, 1)\nax.set_xlabel('$x$')\nax.set_ylabel(r'$-\\ln p$')\nax.grid(False)\npplt.legend(outside='right', frameon=False, title='$f=$')\npplt.show()\n</pre> fig, ax = plt.subplots()  # plot reference potential WPotential1D().plot_potential()  bins = jnp.linspace(0, 1, 32) for ext_force in ext_forces:     hist, _ = np.histogram(         Xs[ext_force],         bins=bins,         density=True,     )      ax.plot(         0.5 * (bins[1:] + bins[:-1]),         -np.log(hist),         label=ext_force,     )  ax.set_xlim(0, 1) ax.set_xlabel('$x$') ax.set_ylabel(r'$-\\ln p$') ax.grid(False) pplt.legend(outside='right', frameon=False, title='$f=$') pplt.show() <pre>&lt;Figure size 200x200 with 0 Axes&gt;</pre> In\u00a0[6]: Copied! <pre>def train_dms(Xs, key, sigma_min, ext_forces):\n    ddms = {}\n    fig, ax = plt.subplots()\n    for ext_force in ext_forces:\n        key, _ = jax.random.split(key)\n        ddm = FPSL(\n            mlp_network=mlp_network,\n            key=key,\n            n_epochs=n_epochs,\n            batch_size=batch_size,\n            sigma_max=sigma_max,\n            sigma_min=sigma_min,\n            fourier_features=fourier_feat,\n            warmup_steps=n_epochs // 10,\n        )\n\n        loss = ddm.train(\n            Xs[ext_force],\n            y=jnp.full((len(Xs[ext_force]), 1), ext_force),\n            lrs=lrs,\n        )['train_loss']\n        ddms[ext_force] = ddm\n\n        ax.plot(np.arange(1, n_epochs + 1), loss)\n\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('loss')\n    pplt.show()\n\n    return ddms, key\n</pre> def train_dms(Xs, key, sigma_min, ext_forces):     ddms = {}     fig, ax = plt.subplots()     for ext_force in ext_forces:         key, _ = jax.random.split(key)         ddm = FPSL(             mlp_network=mlp_network,             key=key,             n_epochs=n_epochs,             batch_size=batch_size,             sigma_max=sigma_max,             sigma_min=sigma_min,             fourier_features=fourier_feat,             warmup_steps=n_epochs // 10,         )          loss = ddm.train(             Xs[ext_force],             y=jnp.full((len(Xs[ext_force]), 1), ext_force),             lrs=lrs,         )['train_loss']         ddms[ext_force] = ddm          ax.plot(np.arange(1, n_epochs + 1), loss)      ax.set_xlabel('epoch')     ax.set_ylabel('loss')     pplt.show()      return ddms, key <p>To visualize the learned denoising score model, we plot the learned energy and score function as functions of the diffusion time and position.</p> In\u00a0[7]: Copied! <pre>def plot_score_energy(ddpm, axs=None):\n    xs = jnp.linspace(0, 1, 50).reshape(-1, 1)\n    ts = jnp.linspace(0, 1, 51)\n\n    if axs is None:\n        fig, axs = plt.subplots(\n            1,\n            2,\n            gridspec_kw={'wspace': 2 / 3},\n        )\n\n    ax = axs[0]\n    score = jnp.array([ddpm.score(xs, t).flatten() for t in ts])\n\n    im = ax.pcolormesh(\n        xs.flatten(),\n        ts,\n        score,\n        norm=cnorm,\n        cmap='Spectral',\n    )\n    pplt.colorbar(im, ax=ax, label=r'$\\nabla \\ln p$')\n    ax.set_title('score')\n\n    ax = axs[1]\n    energy = jnp.array([ddpm.energy(xs, t).flatten() for t in ts])\n    energy -= energy.min(axis=1, keepdims=True)\n    p = np.exp(-energy)\n    p /= p.sum(axis=1, keepdims=True) * (xs[1] - xs[0])\n\n    im = ax.pcolormesh(\n        xs.flatten(),\n        ts,\n        np.log(p),\n        vmin=max(-5, np.log(p.min())),\n    )\n    pplt.colorbar(im, ax=ax, label=r'$\\ln p$')\n    ax.set_title('energy')\n\n    for ax in axs:\n        ax.grid(False)\n        ax.set_xlabel('$x$')\n        ax.set_ylabel('$t$')\n\n    if axs is None:\n        pplt.label_outer(axs=axs)\n        pplt.show()\n\n\ndef plot_score_energy_all(dms):\n    fig, axs = plt.subplots(\n        2,\n        len(dms),\n        gridspec_kw={'wspace': 2 / 3, 'hspace': 1 / 3},\n    )\n\n    for idx, (label, dm) in enumerate(dms.items()):\n        ax_col = axs[:, idx]\n        plot_score_energy(dm, axs=ax_col)\n\n        pplt.text(\n            0.5,\n            1.4,\n            f'$f={label}$' if label != 'all' else 'all',\n            ax=ax_col[0],\n            transform=ax_col[0].transAxes,\n            size='x-large',\n        )\n\n    pplt.label_outer(axs=axs)\n    pplt.show()\n</pre> def plot_score_energy(ddpm, axs=None):     xs = jnp.linspace(0, 1, 50).reshape(-1, 1)     ts = jnp.linspace(0, 1, 51)      if axs is None:         fig, axs = plt.subplots(             1,             2,             gridspec_kw={'wspace': 2 / 3},         )      ax = axs[0]     score = jnp.array([ddpm.score(xs, t).flatten() for t in ts])      im = ax.pcolormesh(         xs.flatten(),         ts,         score,         norm=cnorm,         cmap='Spectral',     )     pplt.colorbar(im, ax=ax, label=r'$\\nabla \\ln p$')     ax.set_title('score')      ax = axs[1]     energy = jnp.array([ddpm.energy(xs, t).flatten() for t in ts])     energy -= energy.min(axis=1, keepdims=True)     p = np.exp(-energy)     p /= p.sum(axis=1, keepdims=True) * (xs[1] - xs[0])      im = ax.pcolormesh(         xs.flatten(),         ts,         np.log(p),         vmin=max(-5, np.log(p.min())),     )     pplt.colorbar(im, ax=ax, label=r'$\\ln p$')     ax.set_title('energy')      for ax in axs:         ax.grid(False)         ax.set_xlabel('$x$')         ax.set_ylabel('$t$')      if axs is None:         pplt.label_outer(axs=axs)         pplt.show()   def plot_score_energy_all(dms):     fig, axs = plt.subplots(         2,         len(dms),         gridspec_kw={'wspace': 2 / 3, 'hspace': 1 / 3},     )      for idx, (label, dm) in enumerate(dms.items()):         ax_col = axs[:, idx]         plot_score_energy(dm, axs=ax_col)          pplt.text(             0.5,             1.4,             f'$f={label}$' if label != 'all' else 'all',             ax=ax_col[0],             transform=ax_col[0].transAxes,             size='x-large',         )      pplt.label_outer(axs=axs)     pplt.show() <p>Finally, we plot the reference energy and the learned energy to visualize the learned free energy landscape.</p> In\u00a0[8]: Copied! <pre>def plot_final_potential(dms, pot):\n    fig, axs = plt.subplots(\n        1,\n        len(dms),\n        gridspec_kw={'wspace': 0},\n        figsize=(1, 1),\n    )\n\n    for idx, (label, dm) in enumerate(dms.items()):\n        plt.sca(axs[idx])\n        pot.plot_potential()\n\n        xs = np.linspace(0, 1, 200)\n        energy = dm.energy(xs.reshape(-1, 1), t=0.0).flatten()\n        p = np.exp(-energy)\n        p /= p.sum() * (xs[1] - xs[0])\n        axs[idx].plot(xs, -np.log(p))\n\n        axs[idx].set_title(f'$f={label}$' if label != 'all' else 'all')\n        axs[idx].set_xlim(0, 0.99)\n\n    pplt.label_outer()\n    pplt.show()\n</pre> def plot_final_potential(dms, pot):     fig, axs = plt.subplots(         1,         len(dms),         gridspec_kw={'wspace': 0},         figsize=(1, 1),     )      for idx, (label, dm) in enumerate(dms.items()):         plt.sca(axs[idx])         pot.plot_potential()          xs = np.linspace(0, 1, 200)         energy = dm.energy(xs.reshape(-1, 1), t=0.0).flatten()         p = np.exp(-energy)         p /= p.sum() * (xs[1] - xs[0])         axs[idx].plot(xs, -np.log(p))          axs[idx].set_title(f'$f={label}$' if label != 'all' else 'all')         axs[idx].set_xlim(0, 0.99)      pplt.label_outer()     pplt.show() In\u00a0[9]: Copied! <pre>dpdms, key = train_dms(Xs, key, sigma_min, ext_forces)\nplot_score_energy_all(dpdms)\nplot_final_potential(dpdms, pot=WPotential1D())\n</pre> dpdms, key = train_dms(Xs, key, sigma_min, ext_forces) plot_score_energy_all(dpdms) plot_final_potential(dpdms, pot=WPotential1D()) <pre>loss=0.2491/0.2488: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:12&lt;00:00,  1.45s/it]\nloss=0.2481/0.2475: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:08&lt;00:00,  1.36s/it]\nloss=0.2585/0.258: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:08&lt;00:00,  1.36s/it] \nloss=0.2851/0.2848: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:08&lt;00:00,  1.36s/it]\n</pre> <pre>&lt;Figure size 200x200 with 0 Axes&gt;</pre> <pre>&lt;Figure size 200x200 with 0 Axes&gt;</pre> <pre>&lt;Figure size 200x200 with 0 Axes&gt;</pre> <p>We find that even with this short training time, all models were able to learn the correct free energy landscape. The learned energy profiles are very close to the reference energy profile. This shows that the FPSL framework is able to learn the free energy landscape from non-equilibrium steady state data. However, what should be noted is that for intermediate diffusion times, these models are not fully converged yet. But since, we are only interested in the equilibrium free energy profile, this is not a problem.</p>"},{"location":"tutorials/fpsl/#fokker-planck-score-learning-fpsl-learning-free-energy-landscapes","title":"Fokker-Planck Score Learning (FPSL): Learning Free Energy Landscapes\u00b6","text":"<p>Fokker--Planck Score Learning (FPSL) is a framework for learning equilibrium free energy profiles from non-equilibrium steady state data. It employs the steady-state solution of the Fokker--Planck equation as an ansatz in the denoising score learning scheme.</p> <p>In the following, we will reproduce the toy model results from the main paper. Just for reference: Using an Nvidia 4060 GTX, running this notebook takes around 5 minutes.</p>"},{"location":"tutorials/fpsl/#imports","title":"Imports\u00b6","text":""},{"location":"tutorials/fpsl/#generate-toy-model-data","title":"Generate Toy Model Data\u00b6","text":"<p>We start here with generating toy model data, to reproduce results from the main paper. Since we only need a training set of positions as external bias forces (typically constant) $(x_t, f_t)$ pairs, this can then be easily extended to MD simulation data.</p>"},{"location":"tutorials/fpsl/#training-the-denoising-score-model","title":"Training the Denoising Score Model\u00b6","text":"<p>In the following we train for each external force a denoising score model (DSM) using the <code>fpsl</code> package. The training is done with the <code>train_dms</code> function, which takes the sampled data and the external forces as input. The training is done for a fixed number of epochs and the loss is plotted.</p>"},{"location":"tutorials/theory/","title":"Theoretical Background","text":""},{"location":"tutorials/theory/#overview","title":"Overview","text":"<p>Fokker-Planck Score Learning (FPSL) is a novel approach that combines the analytical solution of the Fokker-Planck equation for periodic systems with score-based diffusion models to reconstruct free energy landscapes from non-equilibrium data. This method is particularly powerful for systems with periodic boundary conditions, where conventional free energy estimation methods often struggle.</p>"},{"location":"tutorials/theory/#steady-state-solution-of-the-fokker-planck-equation","title":"Steady-State Solution of the Fokker-Planck Equation","text":""},{"location":"tutorials/theory/#non-equilibrium-steady-states-in-periodic-systems","title":"Non-Equilibrium Steady States in Periodic Systems","text":"<p>Consider a Brownian particle in a periodic potential \\(U(x)\\) with period \\(L\\), subject to a constant external driving force \\(f\\). The system is governed by the overdamped Langevin equation:</p> \\[\\frac{dx}{dt} = -\\beta D \\nabla U(x) + f + \\sqrt{2D}\\xi(t)\\] <p>where \\(\\beta = (k_B T)^{-1}\\) is the inverse temperature, \\(D\\) is the diffusion coefficient, and \\(\\xi(t)\\) represents Gaussian white noise.</p> <p>The effective potential under the driving force becomes:</p> \\[U_{\\text{eff}}(x) = U(x) - fx\\]"},{"location":"tutorials/theory/#the-fokker-planck-steady-state-solution","title":"The Fokker-Planck Steady-State Solution","text":"<p>For a periodic system with period \\(L\\), the non-equilibrium steady-state (NESS) distribution has the remarkable analytical form:</p> \\[p^{\\text{s}}(x) \\propto \\frac{1}{D(x)} e^{-\\beta U_{\\text{eff}}(x)} \\int_{x}^{x+L} dy \\, e^{\\beta U_{\\text{eff}}(y)}\\] <p>This expression consists of two key components:</p> <ol> <li>Local Boltzmann factor: the standard equilibrium weighting, \\(e^{-\\beta U_{\\text{eff}}(x)}\\)</li> <li>Periodic correction integral: accounts for the periodic boundary conditions and ensures proper normalization, \\(\\int_{x}^{x+L} dy \\, e^{\\beta U_{\\text{eff}}(y)}\\)</li> </ol> <p>For the derivation of this expression, we refer to our paper on arXiv:2506.15653.</p>"},{"location":"tutorials/theory/#diffusion-models-on-periodic-domains","title":"Diffusion Models on Periodic Domains","text":"<p>Denoising diffusion models learn to generate samples from a target distribution by reversing a gradual noising process. Since we consider periodic systems, using a uniform prior is essential to respect the periodic topology. This leads to the simplified forward process:</p> \\[dx_\\tau = \\sqrt{2\\alpha_\\tau} dW_\\tau\\] <p>where \\(\\tau \\in [0,1]\\) is the diffusion time, \\(\\alpha_\\tau\\) is a noise schedule. With the corresponding reverse process defined as:</p> \\[dx_\\tau = -2\\alpha_\\tau \\nabla \\ln p_\\tau(x_\\tau) d\\tau + \\sqrt{2\\alpha_\\tau} d\\bar{W}_\\tau\\] <p>The key to diffusion models is learning the score function:</p> \\[s(x_\\tau, \\tau) = \\nabla \\ln p_\\tau(x_\\tau)\\] <p>This score guides the reverse diffusion process that transforms noise back into data samples.</p>"},{"location":"tutorials/theory/#fokker-planck-score-learning-the-core-idea","title":"Fokker-Planck Score Learning: The Core Idea","text":""},{"location":"tutorials/theory/#using-physical-insights-as-inductive-bias","title":"Using Physical Insights as Inductive Bias","text":"<p>The central innovation of FPSL is to use the analytical NESS solution as an ansatz for the score function in the diffusion model. Instead of learning the steady-state, we learn the equilibrium distribution from the steady-state samples.</p>"},{"location":"tutorials/theory/#the-fpsl-score-function","title":"The FPSL Score Function","text":"<p>The score function in FPSL combines the standard energy-based score with a periodic correction term:</p> \\[ \\begin{aligned} s^\\theta(x_\\tau, \\tau, L) &amp;= \\nabla \\ln p^{\\text{ss}}(x_\\tau, \\tau, L)\\\\ &amp;= - \\beta \\nabla U^\\theta_{\\text{eff}}(x_\\tau, \\tau) - \\nabla \\ln D(x) + \\Delta s^\\theta(x_\\tau, \\tau, L) \\end{aligned}\\] <p>where the periodic correction is:</p> \\[\\Delta s^\\theta(x_\\tau, \\tau, L) = \\nabla \\ln \\int_{x_\\tau}^{x_\\tau + L} dy \\, e^{\\beta U^\\theta_{\\text{eff}}(y, \\tau)}\\] <p>In the main paper, we show that in our case, this correction is negligible, if we enforce the network to learn a periodic potential.</p> <p>For a more detailed discussion on the periodic correction and its implications, please refer to the main paper.</p>"}]}